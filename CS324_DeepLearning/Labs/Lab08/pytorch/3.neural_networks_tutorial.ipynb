{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "74hzo_1k5kZw"
   },
   "source": [
    "# Neural Networks\n",
    "\n",
    "\n",
    "Neural networks can be constructed using the ``torch.nn`` package.\n",
    "Now that you had a glimpse of ``autograd``, ``nn`` depends on\n",
    "``autograd`` to define models and differentiate them.\n",
    "An ``nn.Module`` contains layers, and a method ``forward(input)`` that\n",
    "returns the ``output``.\n",
    "\n",
    "For example, look at this network that classifies digit images:\n",
    "\n",
    "![convnet](https://pytorch.org/tutorials/_images/mnist.png)\n",
    "   \n",
    "It is a simple feed-forward network. It takes the input, feeds it\n",
    "through several layers one after the other, and then finally gives the\n",
    "output.\n",
    "\n",
    "A typical training procedure for a neural network is as follows:\n",
    "- Define the neural network that has some learnable parameters (or\n",
    "  weights); the default of attribute 'requires_grad' for learnable parameters defined in nn module is True\n",
    "- Iterate over a dataset of inputs\n",
    "- Process input through the network\n",
    "- Compute the loss (how far is the output from being correct)\n",
    "- Propagate gradients back into the network’s parameters\n",
    "- Update the weights of the network, typically using a simple update rule:\n",
    "  ``weight = weight - learning_rate * gradient``\n",
    "\n",
    "## Define the network\n",
    "\n",
    "\n",
    "Let’s define this network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "wDxpcRol5aQ_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lXVi4Ys96m-O"
   },
   "source": [
    "You just have to define the ``forward`` function, and the ``backward``\n",
    "function (where gradients are computed) is automatically defined for you\n",
    "using ``autograd``.\n",
    "\n",
    "You can use any of the Tensor operations in the ``forward`` function.\n",
    "\n",
    "The learnable parameters of a model are returned by ``net.parameters()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[[[ 0.0066,  0.0155,  0.1561,  0.0291, -0.0276],\n",
       "           [-0.1895,  0.0103, -0.1398, -0.1069, -0.1640],\n",
       "           [ 0.1051, -0.0301,  0.0459, -0.0670, -0.1450],\n",
       "           [ 0.0350, -0.1098, -0.1751, -0.0741, -0.0546],\n",
       "           [ 0.0614, -0.0035,  0.1385,  0.0166,  0.1451]]],\n",
       " \n",
       " \n",
       "         [[[-0.0633,  0.0022, -0.1618,  0.1022, -0.1130],\n",
       "           [-0.0901,  0.1911, -0.1918, -0.1058, -0.0850],\n",
       "           [-0.0778, -0.0052,  0.0604,  0.0924, -0.1145],\n",
       "           [-0.1622, -0.0085, -0.0938,  0.0967, -0.0066],\n",
       "           [-0.0552,  0.1720,  0.1555,  0.1007, -0.1201]]],\n",
       " \n",
       " \n",
       "         [[[-0.0938, -0.0739,  0.0497,  0.1318, -0.0628],\n",
       "           [-0.0168,  0.0779,  0.0370, -0.1594,  0.0182],\n",
       "           [ 0.0385, -0.0049,  0.0347, -0.1332, -0.1458],\n",
       "           [ 0.0229,  0.0843,  0.0822, -0.1981, -0.0264],\n",
       "           [ 0.0371, -0.0803, -0.0117, -0.0343, -0.0226]]],\n",
       " \n",
       " \n",
       "         [[[-0.0041,  0.1438, -0.1248,  0.1368,  0.1620],\n",
       "           [-0.1446, -0.1007,  0.1204, -0.0976,  0.1985],\n",
       "           [ 0.0200, -0.0518, -0.0010,  0.1028,  0.1715],\n",
       "           [-0.1533, -0.0996, -0.1206,  0.1490, -0.0431],\n",
       "           [-0.0193, -0.0019, -0.1510, -0.0901,  0.0628]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0021,  0.0023,  0.1923,  0.1169,  0.1086],\n",
       "           [ 0.0771, -0.0874, -0.0410,  0.1941,  0.0036],\n",
       "           [ 0.1738, -0.1897,  0.0980, -0.1256,  0.1372],\n",
       "           [-0.0444,  0.1807, -0.0448,  0.1514, -0.1704],\n",
       "           [ 0.1081, -0.1236, -0.1770,  0.1462,  0.0888]]],\n",
       " \n",
       " \n",
       "         [[[-0.0631, -0.0476,  0.1146,  0.0081, -0.0339],\n",
       "           [ 0.0832, -0.1043, -0.1797, -0.1842, -0.1997],\n",
       "           [-0.1321,  0.1614,  0.1170,  0.1847,  0.0653],\n",
       "           [-0.1086,  0.0599,  0.0063,  0.0586, -0.0502],\n",
       "           [ 0.0662, -0.0276,  0.0391,  0.1073,  0.0373]]]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0305, -0.0154,  0.1077, -0.1090, -0.0851,  0.0606],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[-3.4955e-02,  3.0062e-02,  1.6739e-02, -5.3055e-02, -4.8054e-02],\n",
       "           [ 8.8047e-03, -2.6940e-02,  5.9091e-02,  2.3762e-02, -5.2856e-02],\n",
       "           [-3.6377e-02,  8.1309e-02,  9.1295e-03,  5.1849e-02,  4.2007e-02],\n",
       "           [-6.9228e-02, -1.4543e-02, -7.2833e-02, -2.8990e-02,  6.5844e-02],\n",
       "           [-2.6831e-02,  4.3191e-02, -7.6218e-02,  2.2747e-03, -7.8710e-02]],\n",
       " \n",
       "          [[ 5.8686e-02, -5.9774e-02, -7.4409e-02,  6.8922e-04,  4.4923e-02],\n",
       "           [-3.5211e-02, -2.6590e-02, -5.3366e-02,  3.4613e-02,  1.4231e-02],\n",
       "           [-3.0449e-02,  8.0688e-02, -5.5328e-02,  2.1536e-02, -5.6258e-03],\n",
       "           [ 3.7044e-02, -5.1422e-02, -5.3501e-03,  1.7287e-02, -3.4287e-02],\n",
       "           [ 5.7326e-02, -9.0878e-03,  1.2052e-03, -2.6862e-02,  6.4829e-02]],\n",
       " \n",
       "          [[-6.2878e-02,  3.1922e-02,  6.9542e-02, -3.9847e-02,  2.7873e-03],\n",
       "           [-9.7440e-04, -2.2943e-03, -5.8414e-03, -6.4943e-02,  5.3375e-02],\n",
       "           [ 3.6573e-02, -7.9211e-02,  7.6674e-02,  2.2111e-02, -1.5566e-02],\n",
       "           [-6.2084e-02,  2.4075e-02,  4.1904e-02, -5.1764e-02, -7.7541e-02],\n",
       "           [ 3.7603e-02, -7.6600e-02,  4.5096e-02, -7.4865e-02,  5.8935e-02]],\n",
       " \n",
       "          [[-6.9626e-02,  5.0359e-02, -6.8745e-02,  6.3912e-02, -7.4436e-02],\n",
       "           [ 6.2566e-02, -5.7280e-02,  7.6899e-02,  3.2037e-03, -1.8312e-02],\n",
       "           [ 1.2566e-03,  5.4970e-02, -3.9097e-02,  3.7316e-02, -7.3386e-02],\n",
       "           [ 2.7302e-02, -5.2125e-03, -1.8712e-02, -3.2517e-02, -4.0193e-02],\n",
       "           [ 6.0925e-02, -2.2417e-02, -5.0616e-02, -2.1803e-02,  4.8789e-03]],\n",
       " \n",
       "          [[-4.0638e-02, -7.2007e-02,  4.8702e-02,  1.9279e-02,  1.1936e-02],\n",
       "           [-3.4141e-02,  7.4149e-02,  4.3444e-02, -1.3665e-02,  1.2294e-02],\n",
       "           [-2.6237e-02, -8.0504e-02,  7.8914e-02, -7.5702e-02, -3.4128e-02],\n",
       "           [-3.5700e-02,  3.4587e-02,  5.1104e-02,  7.4287e-02,  6.0022e-02],\n",
       "           [ 1.3704e-02, -5.7087e-02,  6.6964e-02, -7.3540e-02, -4.0971e-02]],\n",
       " \n",
       "          [[-6.0963e-02,  7.2753e-02, -6.0851e-02, -5.1104e-02, -7.2008e-02],\n",
       "           [-2.9612e-02,  2.4657e-02,  4.5306e-02, -5.8631e-02, -5.8582e-02],\n",
       "           [ 1.3282e-02, -2.9836e-02,  2.0101e-02,  1.3173e-02, -3.8534e-02],\n",
       "           [-3.5956e-02, -3.1523e-02,  7.6795e-02,  2.9542e-02, -3.1572e-02],\n",
       "           [-7.3709e-02,  4.4664e-02,  6.9789e-02, -6.8322e-02,  6.1525e-02]]],\n",
       " \n",
       " \n",
       "         [[[-3.5756e-02, -7.6555e-02,  2.6287e-03, -3.1260e-02,  7.4753e-02],\n",
       "           [-1.4938e-02, -8.0020e-02,  2.1223e-02, -7.7784e-02, -7.1038e-02],\n",
       "           [ 4.4503e-02, -6.9005e-02, -5.3477e-03, -2.0050e-02, -5.7829e-02],\n",
       "           [-3.0261e-02, -7.4076e-02, -5.8896e-02, -4.0487e-02, -3.3024e-03],\n",
       "           [ 1.3791e-03, -2.7309e-02, -4.0085e-02, -5.4622e-02,  5.4541e-02]],\n",
       " \n",
       "          [[-7.6772e-02,  2.0706e-03,  2.7245e-03, -4.2653e-02,  3.4354e-02],\n",
       "           [-3.1906e-02,  2.4458e-02,  6.6298e-02,  4.5945e-02, -7.2633e-02],\n",
       "           [ 2.5717e-02, -6.8047e-03,  1.3290e-02,  2.2079e-02,  1.1473e-02],\n",
       "           [ 3.7484e-03,  8.0962e-02, -7.4758e-02,  4.4566e-02,  6.9535e-02],\n",
       "           [ 6.5232e-02,  2.8693e-02, -4.7105e-03,  1.3005e-02, -4.8482e-03]],\n",
       " \n",
       "          [[-7.9810e-02,  4.3198e-04, -5.1239e-02, -6.9935e-02,  7.5400e-02],\n",
       "           [ 1.1996e-02,  3.2661e-02, -1.5919e-02,  5.0708e-02,  5.6073e-02],\n",
       "           [-6.1937e-02, -5.6239e-02, -3.7151e-02,  5.0556e-03, -1.3147e-02],\n",
       "           [-5.2111e-02, -5.7626e-02, -5.9213e-02,  7.8070e-02, -9.7709e-03],\n",
       "           [ 4.9104e-02, -9.9988e-03, -5.6040e-02, -3.2878e-02, -5.3169e-02]],\n",
       " \n",
       "          [[-4.2669e-03, -7.8843e-02,  5.6480e-02,  5.3102e-02, -7.8740e-02],\n",
       "           [-6.1896e-02,  6.9648e-02,  4.3130e-02, -7.8402e-02,  7.2983e-02],\n",
       "           [ 5.1069e-02, -1.4681e-02,  3.8329e-02, -5.0682e-02, -4.8064e-02],\n",
       "           [ 5.4580e-02, -3.1156e-02,  2.1697e-02, -3.4424e-02,  5.3009e-02],\n",
       "           [-3.3024e-02,  7.0241e-02, -1.7825e-02,  1.2865e-02,  6.7174e-02]],\n",
       " \n",
       "          [[-4.5494e-02,  7.6531e-02,  5.8580e-02, -2.3463e-02,  5.9544e-02],\n",
       "           [ 7.5610e-02,  3.7322e-03,  2.1547e-02, -1.9143e-02, -4.8114e-02],\n",
       "           [-7.5115e-03,  1.7771e-03, -2.5495e-02,  1.5253e-02,  1.0787e-02],\n",
       "           [ 5.0663e-03,  7.0124e-02,  5.1589e-02, -2.1333e-02, -2.2998e-02],\n",
       "           [ 6.7660e-02,  5.8156e-02, -5.4679e-02,  6.5443e-02,  3.0614e-02]],\n",
       " \n",
       "          [[ 8.8187e-03,  8.0927e-02,  3.6768e-03,  1.4951e-02, -7.0172e-02],\n",
       "           [-2.2404e-02, -7.5493e-03,  6.3910e-02, -7.2120e-02,  1.7499e-02],\n",
       "           [-7.8692e-02,  9.4608e-03,  5.5631e-02,  6.6933e-02, -7.9763e-02],\n",
       "           [-8.8314e-03,  7.3184e-02,  5.9865e-02, -6.6470e-02, -7.1592e-02],\n",
       "           [-7.3366e-02, -4.7665e-02,  3.6549e-02,  5.2232e-02,  8.1507e-02]]],\n",
       " \n",
       " \n",
       "         [[[-4.8739e-02,  5.3679e-02, -7.1808e-02, -4.7469e-02,  7.1265e-02],\n",
       "           [-1.0829e-02, -5.1084e-03,  3.4245e-03,  1.8118e-02,  5.1710e-02],\n",
       "           [-7.3354e-02,  6.4825e-02, -5.5083e-02,  5.8624e-02, -7.6594e-02],\n",
       "           [-5.6038e-03,  5.6808e-02,  4.5393e-02,  2.4212e-02,  1.8525e-02],\n",
       "           [ 5.2333e-02, -7.8496e-02,  3.8439e-02, -2.3901e-02,  4.5561e-02]],\n",
       " \n",
       "          [[-9.0507e-03,  5.8052e-02,  1.2954e-02, -6.7582e-02, -7.3622e-02],\n",
       "           [ 1.3136e-02, -7.6596e-02, -2.8274e-02,  6.3865e-02, -2.6771e-02],\n",
       "           [-6.7137e-02, -5.3262e-02, -2.1939e-02, -7.1141e-02,  1.9807e-02],\n",
       "           [-2.7660e-02, -6.1298e-03,  4.6950e-02, -7.7963e-02,  2.8711e-02],\n",
       "           [-5.6701e-02,  2.4018e-02, -5.6686e-02,  6.4322e-02, -1.7680e-02]],\n",
       " \n",
       "          [[ 2.2611e-03, -1.1397e-02,  2.6278e-02, -7.6495e-02,  2.3710e-02],\n",
       "           [-2.3610e-03, -2.5239e-02, -7.8956e-02,  5.5958e-02, -6.6305e-02],\n",
       "           [ 5.8418e-02, -4.5835e-03, -3.5434e-02, -3.6463e-02, -7.7441e-02],\n",
       "           [ 1.2432e-02, -1.3477e-02,  4.1372e-02,  2.9757e-02,  3.9944e-02],\n",
       "           [ 4.6383e-03,  2.4596e-02,  4.5058e-02,  1.5075e-02, -1.4247e-02]],\n",
       " \n",
       "          [[ 2.3867e-02, -2.9766e-02,  5.2215e-02,  1.0920e-02,  4.2128e-02],\n",
       "           [ 2.8938e-02, -1.3642e-02, -3.9758e-02, -7.0808e-02, -3.7783e-02],\n",
       "           [-5.3716e-02, -4.1113e-02,  4.7133e-02, -7.8567e-02,  1.3052e-02],\n",
       "           [-6.0614e-02, -3.1815e-02, -7.6737e-02, -2.2931e-02, -6.1065e-03],\n",
       "           [-3.1663e-02, -7.8865e-02, -1.4062e-02, -9.5795e-03,  8.6254e-03]],\n",
       " \n",
       "          [[ 3.6011e-02,  7.9804e-02, -4.7276e-02,  6.4666e-02, -3.0435e-02],\n",
       "           [ 2.2358e-03, -3.1244e-02,  1.5789e-02, -7.8659e-02,  1.0593e-02],\n",
       "           [-9.4758e-03, -3.7253e-02,  4.1710e-02, -4.1329e-02,  2.1208e-02],\n",
       "           [-7.0545e-02, -5.0960e-02, -7.2489e-02, -7.8240e-02, -5.5368e-02],\n",
       "           [-6.7198e-02,  6.6151e-03,  9.4311e-03,  5.5062e-02,  3.8826e-02]],\n",
       " \n",
       "          [[ 1.3029e-02,  2.4450e-02, -5.5997e-02, -4.8238e-02, -7.0047e-02],\n",
       "           [ 2.2181e-02, -4.5068e-02, -1.6444e-02,  1.9458e-02,  2.8369e-02],\n",
       "           [ 5.3748e-03, -2.4405e-02, -4.8232e-02,  1.8015e-02,  4.2455e-02],\n",
       "           [ 2.6694e-02, -5.0447e-02,  7.2151e-02, -3.3806e-02,  5.8017e-02],\n",
       "           [-7.9838e-02,  1.8562e-02, -7.5566e-02, -3.2016e-02,  5.7702e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 8.0905e-02, -1.5838e-03, -1.0908e-02,  6.5680e-02,  6.3436e-02],\n",
       "           [ 3.3588e-02,  3.7368e-02,  2.6953e-02,  2.7146e-02, -4.6662e-02],\n",
       "           [-2.5110e-02,  7.9773e-02,  5.2485e-02,  4.8643e-02,  8.1093e-03],\n",
       "           [ 6.9217e-02, -6.4752e-03, -5.4331e-02, -7.4591e-02, -1.0215e-02],\n",
       "           [ 3.6102e-02,  2.0389e-02,  6.0246e-02, -2.9250e-02, -3.5539e-02]],\n",
       " \n",
       "          [[ 7.4926e-02,  7.7640e-02, -5.0091e-02, -1.4898e-02,  1.8108e-02],\n",
       "           [-6.8335e-02,  7.7123e-02, -8.0638e-02,  7.3759e-02, -3.0131e-02],\n",
       "           [-7.7275e-02,  7.3227e-02, -6.7783e-02,  7.8518e-02,  4.5173e-02],\n",
       "           [ 3.6973e-02,  5.7151e-02,  5.8986e-04,  7.9065e-02, -4.4925e-02],\n",
       "           [ 4.5301e-02,  5.4084e-02,  6.7423e-02,  8.4698e-03,  5.9728e-02]],\n",
       " \n",
       "          [[ 7.0248e-03,  6.2944e-02,  5.2608e-02, -4.5231e-02, -4.4199e-02],\n",
       "           [ 2.8385e-02,  7.3265e-02, -1.8303e-02, -5.4081e-02,  2.1665e-02],\n",
       "           [-2.9431e-02,  2.4129e-02, -6.3436e-02,  5.4847e-03,  7.3877e-02],\n",
       "           [ 6.3999e-02,  6.7996e-02,  2.3890e-02, -1.6000e-02,  3.5683e-02],\n",
       "           [-2.8541e-02, -5.5682e-02, -7.5999e-02, -2.3008e-02,  2.5638e-02]],\n",
       " \n",
       "          [[-2.2723e-02, -2.3126e-02, -2.0405e-03, -3.8767e-02, -3.7167e-02],\n",
       "           [-5.3991e-02, -8.4334e-03, -6.2675e-03,  6.2797e-03, -3.4573e-02],\n",
       "           [-5.2461e-03, -3.8078e-02,  8.1334e-02, -3.5623e-02, -3.3930e-02],\n",
       "           [ 2.8094e-02, -7.2631e-02, -1.9504e-02,  6.0194e-03, -7.7157e-02],\n",
       "           [-7.9772e-02, -4.8188e-02, -4.7644e-02,  2.5575e-02,  6.2660e-02]],\n",
       " \n",
       "          [[-4.6392e-02, -1.4270e-02,  6.0504e-02, -6.2053e-02, -5.0318e-02],\n",
       "           [ 1.0648e-02,  6.0459e-03, -2.8288e-02,  4.1572e-02, -7.6206e-02],\n",
       "           [-7.2546e-02, -1.4096e-02,  5.1642e-02,  4.9336e-02, -6.9982e-02],\n",
       "           [ 5.0460e-02, -5.0209e-02,  4.5394e-02,  3.7057e-02,  5.0882e-02],\n",
       "           [-3.7909e-02,  3.0133e-03,  7.6462e-03,  5.9350e-02, -2.8798e-02]],\n",
       " \n",
       "          [[ 7.6568e-02, -1.7594e-02, -4.5628e-02, -6.7209e-02, -7.8293e-03],\n",
       "           [ 2.4113e-02,  5.4338e-03, -1.9700e-02, -4.9127e-02,  8.0941e-02],\n",
       "           [ 7.9024e-02, -7.7075e-02,  3.8626e-02, -6.4728e-02,  1.8503e-03],\n",
       "           [-5.5839e-02,  4.2818e-02, -7.6085e-02,  6.5719e-03,  1.9603e-02],\n",
       "           [-1.2597e-02,  7.0067e-02,  3.5850e-02,  4.7174e-02,  2.5414e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 4.7649e-02,  7.7469e-04,  1.0384e-02, -1.4692e-02, -1.1166e-02],\n",
       "           [-3.1111e-02,  1.2662e-02,  5.6218e-02, -1.0432e-02, -6.3170e-03],\n",
       "           [-3.0541e-02,  7.9527e-02,  4.1591e-02,  5.1429e-02, -3.9920e-02],\n",
       "           [-1.7206e-02, -1.5796e-02,  5.7560e-02, -7.1344e-02, -9.1845e-03],\n",
       "           [ 4.2462e-02, -1.8904e-03, -8.1476e-02, -4.1242e-02, -8.0917e-02]],\n",
       " \n",
       "          [[-3.8642e-02, -2.8740e-02,  5.8505e-02, -2.7999e-02,  2.8544e-02],\n",
       "           [ 4.7190e-03,  3.3847e-02, -7.0530e-02, -2.0651e-02, -5.3500e-02],\n",
       "           [ 3.5524e-02,  1.4954e-02,  5.8970e-02, -4.6840e-02,  4.2279e-02],\n",
       "           [-4.2593e-02,  1.1379e-04, -2.0725e-02, -2.1111e-02,  8.1454e-02],\n",
       "           [-3.2193e-02, -1.1169e-02, -5.7575e-02, -5.6524e-02,  5.9341e-02]],\n",
       " \n",
       "          [[-3.9566e-02,  3.9113e-02,  6.0440e-02, -7.9788e-02,  4.6426e-03],\n",
       "           [-8.0520e-02,  1.4092e-03,  6.0361e-02,  3.7396e-02,  2.2919e-02],\n",
       "           [-3.4808e-02,  1.6396e-02, -4.8701e-02,  2.6442e-02,  7.9917e-02],\n",
       "           [-3.4225e-02, -4.3977e-02,  3.4129e-02, -4.3546e-02, -3.4290e-02],\n",
       "           [-5.1158e-02,  2.8509e-02,  3.4472e-02, -3.6346e-02, -3.0739e-02]],\n",
       " \n",
       "          [[-7.9017e-02,  3.2064e-03, -4.0268e-02, -7.5995e-02,  7.7694e-02],\n",
       "           [-4.8465e-02, -6.7616e-03,  1.9754e-02,  4.9557e-02,  4.5245e-02],\n",
       "           [-2.7036e-03,  1.3223e-02, -7.2631e-02,  5.1253e-02,  7.5864e-02],\n",
       "           [ 6.6447e-02, -3.9555e-02, -5.0673e-02, -3.0635e-02, -4.1536e-03],\n",
       "           [ 7.1418e-02, -7.8494e-02, -4.5729e-02,  6.1239e-03, -2.2803e-03]],\n",
       " \n",
       "          [[-7.1311e-02, -5.4960e-02,  7.3100e-02, -5.8015e-02, -3.3829e-02],\n",
       "           [ 6.2509e-02,  4.1947e-03,  2.2386e-02,  5.1588e-05,  2.8815e-02],\n",
       "           [ 6.2635e-02, -6.9224e-03,  4.4045e-02,  7.5422e-02, -1.3417e-02],\n",
       "           [-6.7804e-02,  3.7191e-03, -5.1172e-02, -3.7703e-02,  2.4056e-02],\n",
       "           [-7.7687e-03, -7.8939e-02, -4.2384e-02, -4.7261e-02, -1.2446e-02]],\n",
       " \n",
       "          [[ 7.4208e-02, -7.7254e-02, -3.5676e-02,  6.2725e-02,  2.1819e-02],\n",
       "           [-3.4706e-03, -3.7664e-02,  1.3617e-02, -1.2280e-02, -5.8678e-02],\n",
       "           [ 3.9276e-02,  6.3404e-04, -1.5961e-02,  1.0126e-02, -2.0072e-02],\n",
       "           [-5.3112e-03, -2.6950e-02,  1.1997e-02, -1.2636e-02,  1.7504e-02],\n",
       "           [ 6.8299e-05,  6.3673e-03, -2.3800e-02, -6.3176e-02, -2.4975e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 2.1418e-02,  6.8579e-02, -5.3612e-02,  6.3254e-02, -6.0096e-02],\n",
       "           [ 5.6625e-02, -5.2965e-02,  4.0604e-02,  7.5302e-02, -2.9862e-03],\n",
       "           [ 1.2907e-02, -4.9900e-02,  4.6615e-02, -2.9178e-02, -4.0480e-02],\n",
       "           [-1.0057e-02, -3.4077e-02, -2.4665e-02,  9.2140e-03, -1.8579e-02],\n",
       "           [-6.0484e-02, -2.2103e-02,  2.0578e-02, -6.4200e-02, -5.7072e-02]],\n",
       " \n",
       "          [[ 4.1231e-02, -6.2176e-02,  6.6570e-02,  1.1995e-02, -2.2550e-02],\n",
       "           [-3.4417e-02,  5.0775e-02, -9.3281e-03, -6.8206e-02, -7.4168e-02],\n",
       "           [-1.2085e-02, -6.6677e-03, -1.0057e-02,  6.9254e-02, -7.7064e-02],\n",
       "           [ 5.5092e-03, -6.0736e-02,  8.1184e-02,  4.4090e-03,  3.9358e-02],\n",
       "           [-6.3164e-02,  6.4402e-02,  7.6296e-02,  3.4732e-02, -4.1508e-02]],\n",
       " \n",
       "          [[-3.6714e-02, -2.9293e-03, -5.8648e-02,  3.7512e-02, -5.8256e-02],\n",
       "           [ 3.8446e-02,  7.3714e-04, -6.8894e-02,  7.4517e-02, -7.7467e-02],\n",
       "           [-7.2372e-02,  5.3427e-02,  2.1906e-02, -2.5940e-02,  9.9059e-03],\n",
       "           [ 3.8209e-03,  6.1122e-02,  6.5951e-02, -3.8967e-02,  2.7618e-02],\n",
       "           [-6.1626e-02, -3.8780e-03, -4.3491e-02,  7.7516e-05, -7.8628e-02]],\n",
       " \n",
       "          [[-4.1877e-02,  4.4673e-02, -1.7654e-02,  9.0602e-03,  7.7002e-02],\n",
       "           [ 6.6772e-02,  1.1858e-02,  3.6860e-03,  4.6538e-02, -8.1456e-02],\n",
       "           [ 6.8289e-02, -4.7234e-02, -4.8259e-02, -6.3515e-03, -7.9760e-02],\n",
       "           [ 6.4239e-02,  2.9196e-02,  4.0131e-03, -7.9221e-02, -2.9927e-02],\n",
       "           [ 2.7970e-02, -4.2525e-02, -4.8944e-02, -6.6664e-02,  4.7836e-02]],\n",
       " \n",
       "          [[-4.8863e-02, -7.0721e-02,  6.4525e-02, -7.7342e-02,  6.2219e-02],\n",
       "           [ 7.6023e-02, -3.5441e-02,  6.5473e-02, -4.8966e-02,  6.5292e-02],\n",
       "           [ 5.6439e-02, -1.9145e-02,  1.7440e-02,  1.6379e-02, -6.6458e-03],\n",
       "           [ 3.0565e-02, -7.3612e-02,  4.3699e-03,  5.3100e-02,  3.0805e-02],\n",
       "           [-2.6840e-02,  7.7668e-03, -5.9147e-02, -1.0446e-02, -3.0213e-03]],\n",
       " \n",
       "          [[-4.6526e-02, -2.5066e-02,  4.7320e-03,  2.0301e-02, -1.8699e-02],\n",
       "           [ 7.9641e-02,  2.5337e-02, -6.2450e-02, -7.4739e-02, -7.3288e-02],\n",
       "           [ 5.0292e-02, -3.0724e-02, -8.0201e-02,  6.6560e-02, -5.3258e-02],\n",
       "           [ 4.1282e-02,  1.3973e-02, -2.8348e-02, -9.6394e-03, -4.4954e-02],\n",
       "           [-3.4758e-02,  2.0970e-02, -4.6693e-02,  1.2998e-02, -2.9103e-02]]]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0655,  0.0104, -0.0387,  0.0717,  0.0479, -0.0302,  0.0710, -0.0541,\n",
       "         -0.0070, -0.0206,  0.0369, -0.0769, -0.0542, -0.0565,  0.0081, -0.0613],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0264, -0.0155, -0.0037,  ...,  0.0100,  0.0053,  0.0019],\n",
       "         [ 0.0002, -0.0099, -0.0034,  ..., -0.0037,  0.0045, -0.0361],\n",
       "         [ 0.0142, -0.0259,  0.0174,  ...,  0.0390,  0.0141, -0.0139],\n",
       "         ...,\n",
       "         [-0.0375,  0.0268, -0.0304,  ...,  0.0431, -0.0082,  0.0370],\n",
       "         [ 0.0265,  0.0179,  0.0182,  ..., -0.0259,  0.0357, -0.0195],\n",
       "         [ 0.0488, -0.0409, -0.0368,  ..., -0.0096,  0.0129,  0.0033]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0056,  0.0490, -0.0140, -0.0086, -0.0155, -0.0021,  0.0362,  0.0369,\n",
       "          0.0021, -0.0137,  0.0095,  0.0470,  0.0293,  0.0486,  0.0009, -0.0466,\n",
       "          0.0048, -0.0319, -0.0331, -0.0210, -0.0083,  0.0192,  0.0255, -0.0277,\n",
       "         -0.0278,  0.0367, -0.0164,  0.0267, -0.0150, -0.0185,  0.0369, -0.0088,\n",
       "          0.0345,  0.0344,  0.0493,  0.0104,  0.0039, -0.0391,  0.0018,  0.0412,\n",
       "          0.0185, -0.0264, -0.0066, -0.0202,  0.0380, -0.0042, -0.0136, -0.0425,\n",
       "          0.0228, -0.0022,  0.0021,  0.0155, -0.0416,  0.0164, -0.0482,  0.0421,\n",
       "          0.0343,  0.0440,  0.0139,  0.0465,  0.0016, -0.0427,  0.0454, -0.0394,\n",
       "          0.0168,  0.0149, -0.0237,  0.0157,  0.0156, -0.0422,  0.0423,  0.0321,\n",
       "          0.0384,  0.0324,  0.0242,  0.0057, -0.0235, -0.0101,  0.0371,  0.0024,\n",
       "          0.0466, -0.0165, -0.0288, -0.0210, -0.0173,  0.0158, -0.0299, -0.0061,\n",
       "          0.0465,  0.0357,  0.0403,  0.0415, -0.0209, -0.0310, -0.0106, -0.0053,\n",
       "          0.0455, -0.0227,  0.0230, -0.0049, -0.0175,  0.0295, -0.0284, -0.0138,\n",
       "          0.0493, -0.0199,  0.0467, -0.0206,  0.0416, -0.0193,  0.0320, -0.0026,\n",
       "          0.0476, -0.0189,  0.0216,  0.0361,  0.0374, -0.0262,  0.0042,  0.0213],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0538,  0.0244,  0.0179,  ..., -0.0356, -0.0387, -0.0029],\n",
       "         [-0.0543, -0.0460,  0.0699,  ..., -0.0060,  0.0626, -0.0373],\n",
       "         [ 0.0455,  0.0073,  0.0149,  ...,  0.0247, -0.0512, -0.0555],\n",
       "         ...,\n",
       "         [ 0.0338, -0.0459, -0.0790,  ..., -0.0390,  0.0112,  0.0227],\n",
       "         [-0.0230, -0.0641,  0.0901,  ...,  0.0779,  0.0525, -0.0758],\n",
       "         [-0.0772, -0.0291, -0.0822,  ..., -0.0386,  0.0819, -0.0755]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0847,  0.0350, -0.0446, -0.0816, -0.0127,  0.0868,  0.0399,  0.0160,\n",
       "          0.0598,  0.0871, -0.0031, -0.0871,  0.0006, -0.0645, -0.0024,  0.0854,\n",
       "          0.0615, -0.0155,  0.0631,  0.0333,  0.0406, -0.0636,  0.0058,  0.0540,\n",
       "          0.0544,  0.0477, -0.0060, -0.0536, -0.0223,  0.0083, -0.0106,  0.0059,\n",
       "         -0.0192,  0.0290,  0.0720, -0.0698,  0.0758, -0.0404, -0.0146, -0.0412,\n",
       "         -0.0280,  0.0061,  0.0740, -0.0657,  0.0375,  0.0312, -0.0466,  0.0308,\n",
       "          0.0170,  0.0639,  0.0045, -0.0690,  0.0668,  0.0637, -0.0251, -0.0188,\n",
       "          0.0233, -0.0842, -0.0532,  0.0110,  0.0754, -0.0815, -0.0125, -0.0285,\n",
       "         -0.0556,  0.0608, -0.0171, -0.0657, -0.0117,  0.0390, -0.0105, -0.0393,\n",
       "         -0.0682,  0.0013,  0.0308, -0.0850,  0.0298,  0.0109, -0.0635,  0.0814,\n",
       "          0.0729, -0.0755,  0.0475,  0.0888], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0409,  0.0554,  0.0171, -0.0483, -0.0048,  0.0758,  0.0284, -0.0266,\n",
       "          -0.0238,  0.0631, -0.0164, -0.0577,  0.0986,  0.0456, -0.0925, -0.0978,\n",
       "          -0.0087,  0.0623,  0.0452, -0.0801, -0.0829,  0.0606,  0.0494,  0.0876,\n",
       "          -0.1033,  0.0420, -0.0909,  0.0688,  0.0478, -0.0841, -0.0513,  0.0649,\n",
       "          -0.0248, -0.0808,  0.0631,  0.0569, -0.0751, -0.0118, -0.0730, -0.0707,\n",
       "           0.0042, -0.0779, -0.0110, -0.1080,  0.0052,  0.0069, -0.0238, -0.0406,\n",
       "           0.0954,  0.0486, -0.0500,  0.0996, -0.0953, -0.0106,  0.0933,  0.0547,\n",
       "          -0.0628, -0.0297,  0.0984,  0.1065,  0.0338, -0.0539, -0.0164, -0.1043,\n",
       "          -0.0238,  0.1042,  0.0581, -0.0833, -0.0887,  0.0402, -0.0976, -0.1053,\n",
       "          -0.0847, -0.0685,  0.0102,  0.0387,  0.0687,  0.0596,  0.0961,  0.0993,\n",
       "          -0.0999,  0.1018, -0.0064,  0.0046],\n",
       "         [-0.0401, -0.0774,  0.0822,  0.0264, -0.0127,  0.0752,  0.0706,  0.0158,\n",
       "           0.0445,  0.0909, -0.1071,  0.0748,  0.0722,  0.0678,  0.0341, -0.0128,\n",
       "          -0.0274,  0.1055,  0.0756,  0.0995, -0.0472,  0.0473,  0.0367, -0.0912,\n",
       "          -0.0814,  0.0710, -0.0866,  0.0644, -0.0185,  0.0817,  0.0323,  0.0781,\n",
       "           0.0060, -0.0866,  0.0483, -0.0849,  0.1051,  0.1001,  0.0107, -0.0655,\n",
       "          -0.1043, -0.0191, -0.0441,  0.1067,  0.0532,  0.1042,  0.0666,  0.0452,\n",
       "           0.1035,  0.0159,  0.0477, -0.0587, -0.0292,  0.0238, -0.0662, -0.0570,\n",
       "          -0.0169,  0.0648, -0.0531,  0.0585,  0.0393, -0.0658,  0.0250,  0.0732,\n",
       "           0.0412, -0.1015,  0.0781,  0.0867, -0.0285,  0.0896,  0.0488,  0.0742,\n",
       "           0.0382,  0.0252,  0.0079, -0.0390, -0.1051, -0.0136, -0.0934,  0.0704,\n",
       "           0.0518,  0.1077, -0.0540, -0.0625],\n",
       "         [ 0.0779, -0.0236, -0.0320, -0.0453, -0.0500,  0.0277, -0.0377, -0.0179,\n",
       "          -0.0305,  0.0847, -0.0096, -0.0310, -0.0139,  0.0574, -0.0577,  0.0546,\n",
       "           0.0130, -0.0717, -0.0582,  0.0278,  0.0357, -0.0538, -0.0140,  0.0323,\n",
       "          -0.0179, -0.0744,  0.0042, -0.0464,  0.0576, -0.0431, -0.0398, -0.0385,\n",
       "           0.0783,  0.0190,  0.0230, -0.0712,  0.0915,  0.0330, -0.0270, -0.1084,\n",
       "           0.0040, -0.0937, -0.1017, -0.1053,  0.0671,  0.0221, -0.0655, -0.0397,\n",
       "          -0.0432, -0.0573,  0.0409,  0.0846, -0.0196, -0.1068, -0.0888, -0.0310,\n",
       "          -0.0266,  0.0011, -0.0590,  0.0902,  0.0083, -0.0043,  0.0984,  0.0822,\n",
       "           0.0422,  0.0053, -0.1068, -0.0198,  0.0728, -0.0891,  0.1090,  0.0561,\n",
       "          -0.0595,  0.0931, -0.0842, -0.0812,  0.0230, -0.0612, -0.0482,  0.0406,\n",
       "           0.0421,  0.0349,  0.0560, -0.0337],\n",
       "         [ 0.0983, -0.0977, -0.0246,  0.0892, -0.0507,  0.0053, -0.0956, -0.0046,\n",
       "           0.0222, -0.0479, -0.0830,  0.0494, -0.0861,  0.0900, -0.0163,  0.0555,\n",
       "           0.0978, -0.0156, -0.0624, -0.0840, -0.0342,  0.0897,  0.0085, -0.0570,\n",
       "          -0.0953,  0.1048,  0.0318,  0.0635, -0.0391, -0.0373,  0.0768, -0.0764,\n",
       "           0.0205,  0.0225, -0.0981, -0.0798, -0.0587,  0.0137,  0.0733,  0.0806,\n",
       "           0.0980,  0.0225, -0.0706,  0.0962,  0.1084, -0.0234, -0.0803, -0.0880,\n",
       "           0.0093,  0.0850,  0.0047,  0.1012, -0.0436,  0.0059, -0.0939, -0.0633,\n",
       "          -0.0709,  0.0151,  0.0473, -0.0896,  0.0775, -0.0901,  0.0848, -0.0466,\n",
       "          -0.0987, -0.0026,  0.0369, -0.0982,  0.0399,  0.1065, -0.0136,  0.0787,\n",
       "           0.0929,  0.0537,  0.0707,  0.0862,  0.0033,  0.0033,  0.0749, -0.0520,\n",
       "          -0.1072,  0.0776,  0.0280,  0.0795],\n",
       "         [-0.0033, -0.0709,  0.0374,  0.0702, -0.0636,  0.0043,  0.0499,  0.0889,\n",
       "           0.0502, -0.1007, -0.0905, -0.0465,  0.0408, -0.1012, -0.0163,  0.0980,\n",
       "          -0.0379,  0.0572,  0.0850, -0.0316, -0.0673, -0.0084,  0.0454, -0.0073,\n",
       "          -0.0859, -0.0465,  0.0931, -0.0204,  0.0824, -0.0858,  0.0321,  0.0490,\n",
       "           0.0202, -0.0801, -0.0382,  0.1064,  0.0039,  0.0217,  0.0903, -0.0608,\n",
       "          -0.0957,  0.0837, -0.0044,  0.1044,  0.0020,  0.0332,  0.1009,  0.0098,\n",
       "           0.0830, -0.0876, -0.0742,  0.0855,  0.0100,  0.0660,  0.0866, -0.0642,\n",
       "          -0.0474,  0.0407, -0.0201,  0.0552,  0.0730,  0.0213,  0.0857,  0.0519,\n",
       "          -0.0583, -0.0398,  0.0477, -0.0427, -0.0047, -0.0363,  0.0762, -0.0302,\n",
       "          -0.0711,  0.0699,  0.1044, -0.0812,  0.0728,  0.0097,  0.0953,  0.0068,\n",
       "           0.0588,  0.0590, -0.0415,  0.0011],\n",
       "         [-0.0270, -0.0489,  0.0798,  0.0410, -0.0479,  0.1020,  0.1056, -0.0324,\n",
       "          -0.0466,  0.0658, -0.0987, -0.0468, -0.0485,  0.0586,  0.0269,  0.0799,\n",
       "           0.0390, -0.1011, -0.0989,  0.0704,  0.0211, -0.0417, -0.0003,  0.0093,\n",
       "           0.0287, -0.0121, -0.0751, -0.0211,  0.0146, -0.0907, -0.0336, -0.0496,\n",
       "          -0.0539, -0.0558,  0.0953, -0.0632, -0.0650, -0.0173,  0.0945,  0.0257,\n",
       "          -0.0486, -0.0217, -0.0529, -0.0632,  0.0101, -0.0167, -0.0735,  0.0119,\n",
       "          -0.0915,  0.1031,  0.0940, -0.0191,  0.0748, -0.0106,  0.0159,  0.0314,\n",
       "          -0.0838,  0.0820, -0.0121, -0.0664,  0.0274,  0.0907,  0.0385, -0.0365,\n",
       "           0.0129,  0.0714,  0.0454,  0.1047, -0.0299, -0.0795,  0.1088, -0.0650,\n",
       "          -0.0280,  0.0250,  0.0437, -0.0463, -0.0665,  0.0694, -0.0623,  0.0638,\n",
       "          -0.0178,  0.0598, -0.0531, -0.0703],\n",
       "         [-0.0912, -0.0381,  0.0527, -0.0905, -0.0892, -0.0635,  0.1011,  0.0515,\n",
       "           0.0178,  0.0788,  0.0007,  0.0036, -0.0280, -0.0231,  0.0879,  0.0767,\n",
       "          -0.0279, -0.0040,  0.0394,  0.0774,  0.0111,  0.0589,  0.1001,  0.0747,\n",
       "           0.0065,  0.0607,  0.1017, -0.0909, -0.0028, -0.0628,  0.0097,  0.0418,\n",
       "           0.0915, -0.0705,  0.0164, -0.0488,  0.0911, -0.0337,  0.0930,  0.0008,\n",
       "          -0.0274,  0.0608,  0.0021, -0.0185,  0.0491,  0.0771, -0.0632, -0.0295,\n",
       "           0.0750, -0.0976, -0.0746, -0.0148,  0.0842,  0.0665, -0.0782,  0.0241,\n",
       "          -0.0102,  0.0693, -0.0644,  0.0275, -0.1062, -0.0714,  0.0467,  0.0109,\n",
       "           0.0170,  0.0603, -0.0786, -0.0828,  0.0814, -0.0413, -0.0981, -0.0211,\n",
       "           0.0245,  0.0779, -0.0722,  0.0345,  0.1011, -0.0122, -0.0251,  0.0117,\n",
       "           0.0245,  0.0181, -0.0739, -0.0458],\n",
       "         [ 0.0034, -0.0935, -0.0959, -0.0390,  0.0535,  0.0688, -0.0214,  0.1074,\n",
       "           0.0055, -0.0402,  0.0161,  0.0614, -0.0411, -0.0325,  0.0561,  0.0481,\n",
       "          -0.1041, -0.0172,  0.0610,  0.0739,  0.0721, -0.0061, -0.1055, -0.0747,\n",
       "           0.0173,  0.0323,  0.0916, -0.0755, -0.0571, -0.0998, -0.0835, -0.0841,\n",
       "           0.0286,  0.0479,  0.0890,  0.0912,  0.0709,  0.0944,  0.0757, -0.0691,\n",
       "          -0.0231, -0.0771,  0.0222,  0.1035,  0.0814,  0.0992, -0.0165,  0.0791,\n",
       "           0.0664, -0.0795, -0.0103, -0.0233, -0.1030,  0.0947, -0.0615,  0.0642,\n",
       "           0.0016, -0.0979, -0.1001,  0.0812,  0.0899, -0.0297,  0.0245, -0.0695,\n",
       "           0.0673,  0.0437, -0.0661, -0.0997,  0.0652, -0.0296, -0.0086,  0.0119,\n",
       "          -0.0856,  0.0500,  0.0395, -0.0856, -0.0683, -0.0802,  0.0007, -0.0485,\n",
       "          -0.0254,  0.0456,  0.0069,  0.0861],\n",
       "         [ 0.0212, -0.0656,  0.0130, -0.0837,  0.0055,  0.0355,  0.0612, -0.0794,\n",
       "          -0.0074,  0.0133,  0.0457, -0.0109,  0.0446,  0.0865, -0.0640, -0.1038,\n",
       "          -0.0754,  0.0202,  0.0550,  0.0208,  0.0016,  0.0133,  0.0215,  0.0264,\n",
       "          -0.0150,  0.0902,  0.0273,  0.0971, -0.0125,  0.0546, -0.1002, -0.0829,\n",
       "           0.0718,  0.0683, -0.0819,  0.0729, -0.0440, -0.1043,  0.1022,  0.0406,\n",
       "           0.0352, -0.0997,  0.0933, -0.0815,  0.0259,  0.0142,  0.0100, -0.0847,\n",
       "           0.0942, -0.0271, -0.0207,  0.0319,  0.0998,  0.0255,  0.0566,  0.0853,\n",
       "           0.0686,  0.0914, -0.0355,  0.0027, -0.0019,  0.0396,  0.0644,  0.0914,\n",
       "          -0.0991, -0.0724, -0.0829,  0.0930,  0.0080, -0.0440, -0.0412, -0.0979,\n",
       "           0.0789,  0.0625,  0.0257,  0.0876,  0.0794, -0.0380, -0.0124, -0.0810,\n",
       "           0.0799,  0.1069, -0.1053,  0.0276],\n",
       "         [ 0.0214,  0.0079,  0.1066,  0.0702,  0.0464,  0.0367,  0.0101,  0.0032,\n",
       "           0.0770, -0.0546,  0.0972,  0.0256, -0.0497,  0.0891,  0.0180, -0.0275,\n",
       "          -0.0081, -0.0973, -0.1007, -0.0867, -0.0942,  0.0752,  0.0877, -0.0775,\n",
       "           0.0256, -0.0340,  0.0427,  0.0839, -0.0649, -0.0855, -0.0300, -0.0197,\n",
       "          -0.0519, -0.0509,  0.1075, -0.0992, -0.0147,  0.0056, -0.0636,  0.0666,\n",
       "           0.0421, -0.0531,  0.0175,  0.0253, -0.0502, -0.0344,  0.0586, -0.0527,\n",
       "           0.0533, -0.0357, -0.0639, -0.0354, -0.0305,  0.0484, -0.0815,  0.0271,\n",
       "           0.0021, -0.0980,  0.0738, -0.0932,  0.0333,  0.0504, -0.0201,  0.0261,\n",
       "           0.0344, -0.0790, -0.1031, -0.0566, -0.0053, -0.0706, -0.0830,  0.0959,\n",
       "           0.0219,  0.0011,  0.0811,  0.0659,  0.0954,  0.0426,  0.0060, -0.1057,\n",
       "           0.0865,  0.0318,  0.0825,  0.0392]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0242, -0.0265,  0.0725, -0.0406,  0.0507, -0.0504, -0.0464,  0.0198,\n",
       "          0.0175, -0.0189], requires_grad=True)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "r1wVzQtw6nKj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P-uAxqW96nXT"
   },
   "source": [
    "Let try a random 32x32 input\n",
    "\n",
    "Note: Expected input size to this net(LeNet) is 32x32. To use this net on\n",
    "MNIST dataset, please resize the images from the dataset to 32x32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "eL1HjrO_6nlM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0061, -0.0144,  0.0551, -0.0761,  0.0689, -0.0235,  0.0103,  0.0488,\n",
      "          0.0351, -0.0060]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RqsG_zlP6nuY"
   },
   "source": [
    "Zero the gradient buffers of all parameters and backprops with random gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "73bRvqQC6n4F"
   },
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EHQKAqa56oBs"
   },
   "source": [
    "#### note\n",
    "\n",
    "     ``torch.nn`` only supports mini-batches. The entire ``torch.nn``\n",
    "     package only supports inputs that are a mini-batch of samples, and not\n",
    "     a single sample.\n",
    "\n",
    "     For example, ``nn.Conv2d`` will take in a 4D Tensor of\n",
    "     ``nSamples x nChannels x Height x Width``.\n",
    "\n",
    "     If you have a single sample, just use ``input.unsqueeze(0)`` to add\n",
    "     a fake batch dimension.\n",
    "\n",
    "Before proceeding further, let's recap all the classes you’ve seen so far.\n",
    "\n",
    "**Recap:**\n",
    "  -  ``torch.Tensor`` - A *multi-dimensional array* with support for autograd\n",
    "     operations like ``backward()``. Also *holds the gradient* w.r.t. the\n",
    "     tensor.\n",
    "  -  ``nn.Module`` - Neural network module. *Convenient way of\n",
    "     encapsulating parameters*, with helpers for moving them to GPU,\n",
    "     exporting, loading, etc.\n",
    "  -  ``nn.Parameter`` - A kind of Tensor, that is *automatically\n",
    "     registered as a parameter when assigned as an attribute to a*\n",
    "     ``Module``.\n",
    "  -  ``autograd.Function`` - Implements *forward and backward definitions\n",
    "     of an autograd operation*. Every ``Tensor`` operation, creates at\n",
    "     least a single ``Function`` node, that connects to functions that\n",
    "     created a ``Tensor`` and *encodes its history*.\n",
    "\n",
    "**At this point, we covered:**\n",
    "  -  Defining a neural network\n",
    "  -  Processing inputs and calling backward\n",
    "\n",
    "**Still Left:**\n",
    "  -  Computing the loss\n",
    "  -  Updating the weights of the network\n",
    "\n",
    "Loss Function\n",
    "-------------\n",
    "\n",
    "A loss function takes the (output, target) pair of inputs, and computes a\n",
    "value that estimates how far away the output is from the target.\n",
    "\n",
    "There are several different\n",
    "[loss functions](http://pytorch.org/docs/nn.html#loss-functions) under the\n",
    "nn package .\n",
    "A simple loss is: ``nn.MSELoss`` which computes the mean-squared error\n",
    "between the input and the target.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "spzKkq5j6oMZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8191, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k9oHzF8Y6oWG"
   },
   "source": [
    "Now, if you follow ``loss`` in the backward direction, using its\n",
    "``.grad_fn`` attribute, you will see a graph of computations that looks\n",
    "like this:\n",
    "\n",
    "\n",
    "     input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "           -> view -> linear -> relu -> linear -> relu -> linear\n",
    "           -> MSELoss\n",
    "           -> loss\n",
    "\n",
    "So, when we call ``loss.backward()``, the whole graph is differentiated\n",
    "w.r.t. the loss, and all Tensors in the graph that has ``requires_grad=True``\n",
    "will have their ``.grad`` Tensor accumulated with the gradient.\n",
    "\n",
    "For illustration, let us follow a few steps backward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "89alWFKt6ogd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward0 object at 0x0000028F15948580>\n",
      "<AddmmBackward0 object at 0x0000028F1594A3B0>\n",
      "<AccumulateGrad object at 0x0000028F15948580>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F8xxhpyr6oqE"
   },
   "source": [
    "## Backprop\n",
    "\n",
    "To backpropagate the error all we have to do is to ``loss.backward()``.\n",
    "You need to clear the existing gradients though, else gradients will be\n",
    "accumulated to existing gradients.\n",
    "\n",
    "\n",
    "Now we shall call ``loss.backward()``, and have a look at conv1's bias\n",
    "gradients before and after the backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0galcsNB6o0h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "None\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0146, -0.0038,  0.0089,  0.0005, -0.0012, -0.0051])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YS6LKjtn6o9u"
   },
   "source": [
    "Now, we have seen how to use loss functions.\n",
    "\n",
    "**Read Later:**\n",
    "\n",
    "  The neural network package contains various modules and loss functions\n",
    "  that form the building blocks of deep neural networks. A full list with\n",
    "  documentation is [here](http://pytorch.org/docs/nn).\n",
    "\n",
    "**The only thing left to learn is:**\n",
    "\n",
    "  - Updating the weights of the network\n",
    "\n",
    "Update the weights\n",
    "------------------\n",
    "The simplest update rule used in practice is the Stochastic Gradient\n",
    "Descent (SGD):\n",
    "\n",
    "     weight = weight - learning_rate * gradient\n",
    "\n",
    "We can implement this using simple python code:\n",
    "\n",
    "\n",
    "```python\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)\n",
    "```\n",
    "\n",
    "However, as you use neural networks, you want to use various different\n",
    "update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc.\n",
    "To enable this, we built a small package: ``torch.optim`` that\n",
    "implements all these methods. Using it is very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tdnVLdHo6pI1"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K2JGjDs_6pRh"
   },
   "source": [
    "#### Note\n",
    "\n",
    "      Observe how gradient buffers had to be manually set to zero using\n",
    "      ``optimizer.zero_grad()``. This is because gradients are accumulated\n",
    "      as explained in Backprop section."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "neural_networks_tutorial.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "cs324",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
