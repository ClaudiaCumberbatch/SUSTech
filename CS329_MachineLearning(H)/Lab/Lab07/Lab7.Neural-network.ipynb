{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB7 tutorial for Machine Learning <br >Neural NetWork & Pytorch\n",
    "> The document description are designed by JIa Yanhong in 2022. Oct. 20th\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "- Install the pytorch neural network framework for your computer.\n",
    "- Learn to use pytorch.\n",
    "- Implement a simple neural network using pytorch\n",
    "- Complete the LAB assignment.\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "Neural networks, also known as artificial neural networks (ANNs) or simulated neural networks (SNNs), are a subset of  machine learning and are at the heart of  deep learning algorithms. Their name and structure are inspired by the human brain, mimicking the way that biological neurons signal to one another.\n",
    "\n",
    "![img](images/1hkYlTODpjJgo32DoCOWN5w.png)\n",
    "\n",
    "The above neural could be represented as:\n",
    "$$y=f(b+\\sum_{i=1}^{n}w_ix_i)=f(w^Tx)$$\n",
    "\n",
    "Artificial neural networks (ANNs) are comprised of a node layers, containing `an input layer`, `one or more hidden layers`, and `an output layer`. Each node, or artificial neuron, connects to another and has an associated `weight` and `threshold`. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.\n",
    "\n",
    "<img src=\"images/ICLH_Diagram_Batch_01_03-DeepNeuralNetwork-WHITEBG.png\" alt=\"Visual diagram of an input layer, hidden layers, and an output layer of a feedforward neural network \" style=\"zoom:40%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The role of neural networks in Machine learning\n",
    "+ Supervise machine learning process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Supervise machine learning process ](images/Supervise-machine-learning-process.png)\n",
    "\n",
    "  In the figure above, the hardest part is how to obtain valid feature vectors. This is a technique called **feature engineering**.\n",
    "\n",
    "+ **The Importance of Feature Engineering:**\n",
    "\n",
    "  + Preprocessing and feature extraction determine the upper bound of the model\n",
    "  + The algorithm and parameter selection approach this upper bound.\n",
    "\n",
    "+ **Traditional feature extraction methods:**\n",
    "\n",
    "  <img src=\"images/image-20221020212446504.png\" alt=\"image-20221020212446504 \" style=\"zoom:60%;\" />\n",
    "\n",
    "+ **Neural networks automatically extract features**\n",
    "  \n",
    "  <img src=\"images/image-20221020212805853.png\" alt=\"image-20221020212805853 \" style=\"zoom:60%;\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch\n",
    "### Install Pytorch\n",
    "Please refer to **Installing PyTorch on Windows 10.md** for this section:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning pytorch with linear regression\n",
    "#### Linear regression\n",
    "Do you remember the general form of linear regression model's prediction?\n",
    "$$\\hat{y}=h_{\\theta }(x)=\\theta _{0}+\\theta _{1}x_{1}+\\theta _{2}x_{2}+...+\\theta _{n}x_{n}=\\theta ^{T}\\cdot x$$\n",
    "\n",
    "<center>\n",
    "    <img src='images/fit-linreg.svg' style=\"zoom:100%;\"/>\n",
    "    <br>\n",
    "    <div style=\"\">\n",
    "       Fitting a linear regression model to one-dimensional data\n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "Linear regression is a single-layer neural network, We used this simple network to learn how to use pytorch.\n",
    "<center>\n",
    "    <img src='images/singleneuron.svg' style=\"zoom:100%;\"/>\n",
    "    <br>\n",
    "    <div style=\"\">\n",
    "        Linear regression is a single-layer neural network \n",
    "    </div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Warm-up: [numpy](https://numpy.org/learn/)\n",
    "Before introducing PyTorch, we will first implement the network using numpy.\n",
    "\n",
    "Numpy provides an n-dimensional array object, and many functions for manipulating these arrays. Numpy is a generic framework for scientific computing; it does not know anything about computation graphs, or deep learning, or gradients. However we can easily use numpy to fit a third order polynomial to sine function by manually implementing the forward and backward passes through the network using numpy operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x114b47d30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "# Create random input and output data\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "\n",
    "plt.scatter(x,y,c = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 249.121276855919\n",
      "199 167.71686756875715\n",
      "299 113.89043358540466\n",
      "399 78.29745439691573\n",
      "499 54.761421587661815\n",
      "599 39.198096746789226\n",
      "699 28.906761660023818\n",
      "799 22.101555499135124\n",
      "899 17.60157014149687\n",
      "999 14.625924723698489\n",
      "1099 12.658258007151389\n",
      "1199 11.357123476841132\n",
      "1299 10.496737765621107\n",
      "1399 9.927800315203555\n",
      "1499 9.551585212525827\n",
      "1599 9.302809301177689\n",
      "1699 9.138303639110722\n",
      "1799 9.029522443191965\n",
      "1899 8.957589588848158\n",
      "1999 8.910023079884127\n",
      "Result: y = -0.00018957054425228413 + 0.847375179528296 x + 3.270407335098956e-05 x^2 + -0.09199820495816882 x^3\n"
     ]
    }
   ],
   "source": [
    "# Randomly initialize weights\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    # y = a + b x + c x^2 + d x^3\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch: Tensors \n",
    "Numpy is a great framework, but it cannot utilize GPUs to accelerate its numerical computations. For modern deep neural networks, GPUs often provide speedups of 50x or greater, so unfortunately numpy won’t be enough for modern deep learning.\n",
    "\n",
    "Here we introduce the most fundamental PyTorch concept: the Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2957.228759765625\n",
      "199 1980.5704345703125\n",
      "299 1328.291015625\n",
      "399 892.39501953125\n",
      "499 600.9207763671875\n",
      "599 405.89227294921875\n",
      "699 275.30810546875\n",
      "799 187.81185913085938\n",
      "899 129.14303588867188\n",
      "999 89.77349853515625\n",
      "1099 63.33379364013672\n",
      "1199 45.56282043457031\n",
      "1299 33.60816192626953\n",
      "1399 25.559009552001953\n",
      "1499 20.134498596191406\n",
      "1599 16.47545051574707\n",
      "1699 14.004807472229004\n",
      "1799 12.334980010986328\n",
      "1899 11.205198287963867\n",
      "1999 10.440065383911133\n",
      "Result: y = 0.02735675498843193 + 0.8267014622688293 x + -0.004719496238976717 x^2 + -0.0890575498342514 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch: Tensors and autograd\n",
    "In the above examples, we had to manually implement both the forward and backward passes of our neural network. Manually implementing the backward pass is not a big deal for a small two-layer network, but can quickly get very hairy for large complex networks.\n",
    "\n",
    "Thankfully, we can use automatic differentiation to automate the computation of backward passes in neural networks. The autograd package in PyTorch provides exactly this functionality. \n",
    "\n",
    "When using autograd, the forward pass of your network will define a computational graph; nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. Backpropagating through this graph then allows you to easily compute gradients.\n",
    "\n",
    "This sounds complicated, it’s pretty simple to use in practice. Each Tensor represents a node in a computational graph. If `x` is a Tensor that has `x.requires_grad=True` then `x.grad` is another Tensor holding the gradient of `x` with respect to some scalar value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 645.0086669921875\n",
      "199 445.8989562988281\n",
      "299 309.4453430175781\n",
      "399 215.82192993164062\n",
      "499 151.51197814941406\n",
      "599 107.28758239746094\n",
      "699 76.84173583984375\n",
      "799 55.858402252197266\n",
      "899 41.380889892578125\n",
      "999 31.38153076171875\n",
      "1099 24.467857360839844\n",
      "1199 19.6827392578125\n",
      "1299 16.36758041381836\n",
      "1399 14.068577766418457\n",
      "1499 12.472726821899414\n",
      "1599 11.363958358764648\n",
      "1699 10.59290599822998\n",
      "1799 10.056253433227539\n",
      "1899 9.682428359985352\n",
      "1999 9.421807289123535\n",
      "Result: y = -0.023559732362627983 + 0.8465813994407654 x + 0.00406444538384676 x^2 + -0.0918852910399437 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 自动求导\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.data.item())\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    # grad_y_pred = 2.0 * (y_pred - y)\n",
    "    # grad_a = grad_y_pred.sum()\n",
    "    # grad_b = (grad_y_pred * x).sum()\n",
    "    # grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    # grad_d = (grad_y_pred * x ** 3).sum()\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    a.data -= learning_rate * a.grad.data\n",
    "    b.data -= learning_rate * b.grad.data\n",
    "    c.data -= learning_rate * c.grad.data\n",
    "    d.data -= learning_rate * d.grad.data\n",
    "\n",
    "    a.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "    c.grad.data.zero_()\n",
    "    d.grad.data.zero_()\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.data.item()} + {b.data.item()} x + {c.data.item()} x^2 + {d.data.item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch: [nn module](https://pytorch.org/docs/stable/nn.html)\n",
    "we use the nn package to implement our polynomial model network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Using torch.nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 434.15374755859375\n",
      "199 292.6296081542969\n",
      "299 198.29884338378906\n",
      "399 135.39328002929688\n",
      "499 93.4223403930664\n",
      "599 65.40403747558594\n",
      "699 46.68935012817383\n",
      "799 34.18154525756836\n",
      "899 25.816707611083984\n",
      "999 20.219083786010742\n",
      "1099 16.47057342529297\n",
      "1199 13.958562850952148\n",
      "1299 12.273941040039062\n",
      "1399 11.143292427062988\n",
      "1499 10.38387680053711\n",
      "1599 9.873358726501465\n",
      "1699 9.529874801635742\n",
      "1799 9.298563957214355\n",
      "1899 9.142650604248047\n",
      "1999 9.03746223449707\n",
      "Result: y = 0.009304680861532688 + 0.8451155424118042 x + -0.0016052118735387921 x^2 + -0.09167678654193878 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# For this example, the output y is a linear function of (x, x^2, x^3), so\n",
    "# we can consider it as a linear layer neural network. Let's prepare the\n",
    "# tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n",
    "# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n",
    "# of shape (2000, 3) \n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. The Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "# The Flatten layer flatens the output of the linear layer to a 1D tensor,\n",
    "# to match the shape of `y`.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "\n",
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model[0]\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Define the class\n",
    "  \n",
    "We define our neural network by subclassing `nn.Module`, and initialize the neural network layers in __init__. Every `nn.Module` subclass implements the operations on input data in the `forward()` method.\n",
    "The `forward()` method is in charge of conducting the **forward propagation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__()       \n",
    "        self.linear = nn.Linear(3, 1)\n",
    "        self.flatten = nn.Flatten(0, 1)\n",
    "#       self.model = torch.nn.Sequential(\n",
    "#     torch.nn.Linear(3, 1),\n",
    "#     torch.nn.Flatten(0, 1)\n",
    "# )\n",
    "    # 定义前向传播，自动后向传播\n",
    "    def forward(self, x):\n",
    "        y = self.flatten(self.linear(x))\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 567.1124877929688\n",
      "199 379.4937438964844\n",
      "299 254.99163818359375\n",
      "399 172.3533172607422\n",
      "499 117.48863983154297\n",
      "599 81.05329132080078\n",
      "699 56.850276947021484\n",
      "799 40.76789474487305\n",
      "899 30.078231811523438\n",
      "999 22.970703125\n",
      "1099 18.243125915527344\n",
      "1199 15.097458839416504\n",
      "1299 13.003539085388184\n",
      "1399 11.609124183654785\n",
      "1499 10.680150985717773\n",
      "1599 10.060943603515625\n",
      "1699 9.648030281066895\n",
      "1799 9.372529983520508\n",
      "1899 9.188612937927246\n",
      "1999 9.06576919555664\n",
      "Result: y = 0.0071338447742164135 + 0.8428846597671509 x + -0.0012307064607739449 x^2 + -0.0913594663143158 x^3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# For this example, the output y is a linear function of (x, x^2, x^3), so\n",
    "# we can consider it as a linear layer neural network. Let's prepare the\n",
    "# tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n",
    "# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n",
    "# of shape (2000, 3) \n",
    "\n",
    "\n",
    "model = LinearModel()\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "for t in range(2000):\n",
    "\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "\n",
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model.linear\n",
    "\n",
    "\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch: optim\n",
    "Up to this point we have updated the weights of our models by manually mutating the Tensors holding learnable parameters with `torch.no_grad()`. This is not a huge burden for simple optimization algorithms like stochastic gradient descent, but in practice we often train neural networks using more sophisticated optimizers like AdaGrad, RMSProp, Adam, etc.\n",
    "\n",
    "The `optim` package in PyTorch abstracts the idea of an optimization algorithm and provides implementations of commonly used optimization algorithms.\n",
    "\n",
    "In this example we will use the `nn` package to define our model as before, but we will optimize the model using the `RMSprop` algorithm provided by the `optim` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 3426.287841796875\n",
      "199 2374.188720703125\n",
      "299 2061.404052734375\n",
      "399 1781.0205078125\n",
      "499 1522.5321044921875\n",
      "599 1288.900146484375\n",
      "699 1078.94189453125\n",
      "799 890.9622802734375\n",
      "899 723.6351318359375\n",
      "999 576.0109252929688\n",
      "1099 447.3831481933594\n",
      "1199 337.148681640625\n",
      "1299 244.79391479492188\n",
      "1399 169.6470184326172\n",
      "1499 111.00283813476562\n",
      "1599 67.46932983398438\n",
      "1699 37.890350341796875\n",
      "1799 20.33914566040039\n",
      "1899 12.031425476074219\n",
      "1999 9.373124122619629\n",
      "Result: y = -0.0005057855159975588 + 0.8360777497291565 x + -0.0005057841190136969 x^2 + -0.09094426780939102 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# For this example, the output y is a linear function of (x, x^2, x^3), so\n",
    "# we can consider it as a linear layer neural network. Let's prepare the\n",
    "# tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n",
    "# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n",
    "# of shape (2000, 3) \n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. The Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "# The Flatten layer flatens the output of the linear layer to a 1D tensor,\n",
    "# to match the shape of `y`.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "model.requires_grad_()\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "optimizer = torch.optim.RMSprop(params=model.parameters(),lr=0.001)\n",
    "#optimizer = torch.optim.SGD(model.parameters(),lr=1e-6,momentum=0.9)\n",
    "\n",
    "for t in range(2000):\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass: compute predicted y by passing x to the model. \n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "   \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())    \n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. \n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model[0]\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning pytorch with logistic regression\n",
    "If we use a single-layer network for classification, this is known as a logistic regression.\n",
    "\n",
    "\n",
    " We need to add the sigmoid function to the output of the linear regression.\n",
    "<center>\n",
    "    <img src='images/Center.png' style=\"zoom:100%;\"/>\n",
    "    <br>\n",
    "    <div style=\"\">\n",
    "       Perceptron\n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "Let us define the number of epochs and the learning rate we want our model for training. As the data is a binary  classification, we will use **Binary Cross Entropy** as the **loss function** used to optimize the model using an `SGD optimizer`.\n",
    "\n",
    "<font size=5 color='red'>Please complete this part of the code!!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "### Perceptron-The basic unit of neural network\n",
    "A simple model of a biological neuron in an artificial neural network is known as Perceptron. it is the primary step to learn Machine Learning and Deep Learning technologies.\n",
    "\n",
    "we can consider it as a single-layer neural network with four main parameters, i.e., `input values`, `weights and Bias`, `net sum`, and an `activation function`.\n",
    "\n",
    "![Perceptron in Machine Learning](images/perceptron-in-machine-learning2.png)\n",
    "\n",
    "- **Input Nodes or Input Layer:**\n",
    "\n",
    "This is the primary component of Perceptron which accepts the initial data into the system for further processing.\n",
    "\n",
    "- **Wight and Bias:**\n",
    "\n",
    "Weight parameter represents the strength of the connection between units.  Bias can be considered as the intercept in a linear equation.\n",
    "\n",
    "- **Activation Function:**\n",
    "\n",
    "These are the final and important components that help to determine whether the neuron will fire or not. The activation function of perceptron is `sign function`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron(MLP)\n",
    "A neuron is a mathematical model of the behaviour of a single neuron in a biological nervous system.\n",
    "\n",
    "A single neuron can solve some simple tasks, but the power of neural networks comes when many of them are arranged in layers and connected in a network architecture.\n",
    "\n",
    "<img src=\"images/multilayer-perceptron-1.png\" alt=\"multilayer-perceptron-1 \" style=\"zoom:40%;\" />\n",
    "\n",
    "\n",
    "**A Multilayered Perceptron is a Neural Network**. A neural network having more than 3 hidden layers is called a **Deep Neural Network**.\n",
    "\n",
    "In this lab, Multilayer Perceptron and Neural Network  mean the same thing.\n",
    "\n",
    "------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "Various activation functions that can be used with Perceptron.\n",
    "\n",
    "![Perceptron_36.](images/Perceptron_36.jpg)\n",
    "\n",
    "<font color=\"red\">Neural network without activation functions are simply linear regression model</font>. The activation makes the input capable of learning and performing more complex tasks.\n",
    "\n",
    "![image-20221023014216170](images/image-20221023014216170.png)\n",
    "\n",
    "Therefore, when we write the neural network framework, the neurons in each hidden layer are most of the time **followed by an activation function**.\n",
    "\n",
    "I recommend that you use the relu function as you build your neural network framework.\n",
    "\n",
    "![image-20221023015025204](images/image-20221023015025204.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example for MLP\n",
    "This Example uses dataset digit123.csv , which has 36 columns, and the last column is the dependent variable. We use this dataset to familiarize ourselves with MLP and solve the multi-classification problem.\n",
    "\n",
    "\n",
    "**Note that the values of the dependent variable are 1,2,3, and label coding is required.**\n",
    "#### MLP Model \n",
    "\n",
    "+ step 1 load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   ...  27  28  29  30  31  32  33  \\\n",
       "0   0   0   0   1   0   0   0   0   0   1  ...   1   0   0   0   0   0   1   \n",
       "1   0   0   0   1   0   0   0   0   1   1  ...   1   0   0   0   0   0   1   \n",
       "2   0   0   1   1   0   0   0   0   0   1  ...   1   0   0   0   0   0   1   \n",
       "3   0   0   0   1   0   0   0   0   0   1  ...   1   0   0   0   0   1   1   \n",
       "4   0   0   0   1   0   0   0   0   0   1  ...   1   0   0   0   0   1   1   \n",
       "\n",
       "   34  35  36  \n",
       "0   0   0   1  \n",
       "1   0   0   1  \n",
       "2   0   0   1  \n",
       "3   0   0   1  \n",
       "4   1   0   1  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 'Matplotlib' is a data visualization library for 2D and 3D plots, built on numpy\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# to suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "# ============================ step 1/6 load datasets ============================\n",
    "df = pd.read_csv(\"datasets/digit123.csv\", header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36\n",
       "1    32\n",
       "2    32\n",
       "3    32\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[36].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 36)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df[36]\n",
    "y.replace((1, 2, 3),(0, 1, 2),inplace=True)\n",
    "X = df.drop(36, axis=1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting for X and Y variables:\n",
    "from sklearn.model_selection import train_test_split\n",
    "## Splitting dataset into 80% Training and 20% Testing Data:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.8, random_state =0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torch \n",
    "#Converting them to tensors as PyTorch works on, we will use the torch.from_numpy() method:\n",
    "X_train = torch.from_numpy(X_train.values).float()\n",
    "X_test = torch.from_numpy(X_test.values).float()\n",
    "y_train = torch.from_numpy(y_train.values).long()\n",
    "y_test = torch.from_numpy(y_test.values).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ step 2 Define a MLP subclass of nn. Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a MLP subclass of nn. Module.\n",
    "# ============================ step 2/6 define model ============================\n",
    "import  torch \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_i, n_h, n_o):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(n_i, n_h)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(n_h, n_o)\n",
    "    def forward(self, input):\n",
    "        return self.linear2(self.relu(self.linear1(self.flatten(input))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ step 3 Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 10000\n",
    "learning_rate = 0.001 \n",
    "# Create the model\n",
    "# ============================ step 3/6 Create model ============================\n",
    "models = MLP(X_train.shape[1],X_train.shape[1]//2,y_train.unique().size()[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ step 4 Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ step 4/6 Loss function ============================\n",
    "criterions = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ step 5 The optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ step 4/6 The optimizer ============================\n",
    "optimizers = torch.optim.SGD(models.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ step 6 Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, loss = 1.0614\n",
      "epoch: 40, loss = 1.0581\n",
      "epoch: 60, loss = 1.0547\n",
      "epoch: 80, loss = 1.0515\n",
      "epoch: 100, loss = 1.0483\n",
      "epoch: 120, loss = 1.0451\n",
      "epoch: 140, loss = 1.0420\n",
      "epoch: 160, loss = 1.0388\n",
      "epoch: 180, loss = 1.0357\n",
      "epoch: 200, loss = 1.0326\n",
      "epoch: 220, loss = 1.0294\n",
      "epoch: 240, loss = 1.0263\n",
      "epoch: 260, loss = 1.0231\n",
      "epoch: 280, loss = 1.0199\n",
      "epoch: 300, loss = 1.0167\n",
      "epoch: 320, loss = 1.0135\n",
      "epoch: 340, loss = 1.0103\n",
      "epoch: 360, loss = 1.0071\n",
      "epoch: 380, loss = 1.0039\n",
      "epoch: 400, loss = 1.0007\n",
      "epoch: 420, loss = 0.9975\n",
      "epoch: 440, loss = 0.9942\n",
      "epoch: 460, loss = 0.9910\n",
      "epoch: 480, loss = 0.9878\n",
      "epoch: 500, loss = 0.9845\n",
      "epoch: 520, loss = 0.9813\n",
      "epoch: 540, loss = 0.9780\n",
      "epoch: 560, loss = 0.9746\n",
      "epoch: 580, loss = 0.9712\n",
      "epoch: 600, loss = 0.9679\n",
      "epoch: 620, loss = 0.9646\n",
      "epoch: 640, loss = 0.9612\n",
      "epoch: 660, loss = 0.9578\n",
      "epoch: 680, loss = 0.9544\n",
      "epoch: 700, loss = 0.9510\n",
      "epoch: 720, loss = 0.9476\n",
      "epoch: 740, loss = 0.9442\n",
      "epoch: 760, loss = 0.9408\n",
      "epoch: 780, loss = 0.9373\n",
      "epoch: 800, loss = 0.9338\n",
      "epoch: 820, loss = 0.9303\n",
      "epoch: 840, loss = 0.9268\n",
      "epoch: 860, loss = 0.9233\n",
      "epoch: 880, loss = 0.9197\n",
      "epoch: 900, loss = 0.9161\n",
      "epoch: 920, loss = 0.9125\n",
      "epoch: 940, loss = 0.9088\n",
      "epoch: 960, loss = 0.9051\n",
      "epoch: 980, loss = 0.9014\n",
      "epoch: 1000, loss = 0.8977\n",
      "epoch: 1020, loss = 0.8939\n",
      "epoch: 1040, loss = 0.8901\n",
      "epoch: 1060, loss = 0.8863\n",
      "epoch: 1080, loss = 0.8824\n",
      "epoch: 1100, loss = 0.8785\n",
      "epoch: 1120, loss = 0.8746\n",
      "epoch: 1140, loss = 0.8706\n",
      "epoch: 1160, loss = 0.8666\n",
      "epoch: 1180, loss = 0.8626\n",
      "epoch: 1200, loss = 0.8585\n",
      "epoch: 1220, loss = 0.8544\n",
      "epoch: 1240, loss = 0.8502\n",
      "epoch: 1260, loss = 0.8461\n",
      "epoch: 1280, loss = 0.8419\n",
      "epoch: 1300, loss = 0.8377\n",
      "epoch: 1320, loss = 0.8335\n",
      "epoch: 1340, loss = 0.8294\n",
      "epoch: 1360, loss = 0.8252\n",
      "epoch: 1380, loss = 0.8210\n",
      "epoch: 1400, loss = 0.8168\n",
      "epoch: 1420, loss = 0.8125\n",
      "epoch: 1440, loss = 0.8082\n",
      "epoch: 1460, loss = 0.8038\n",
      "epoch: 1480, loss = 0.7995\n",
      "epoch: 1500, loss = 0.7952\n",
      "epoch: 1520, loss = 0.7908\n",
      "epoch: 1540, loss = 0.7865\n",
      "epoch: 1560, loss = 0.7821\n",
      "epoch: 1580, loss = 0.7778\n",
      "epoch: 1600, loss = 0.7734\n",
      "epoch: 1620, loss = 0.7690\n",
      "epoch: 1640, loss = 0.7646\n",
      "epoch: 1660, loss = 0.7602\n",
      "epoch: 1680, loss = 0.7557\n",
      "epoch: 1700, loss = 0.7513\n",
      "epoch: 1720, loss = 0.7468\n",
      "epoch: 1740, loss = 0.7423\n",
      "epoch: 1760, loss = 0.7378\n",
      "epoch: 1780, loss = 0.7333\n",
      "epoch: 1800, loss = 0.7288\n",
      "epoch: 1820, loss = 0.7243\n",
      "epoch: 1840, loss = 0.7197\n",
      "epoch: 1860, loss = 0.7152\n",
      "epoch: 1880, loss = 0.7106\n",
      "epoch: 1900, loss = 0.7060\n",
      "epoch: 1920, loss = 0.7015\n",
      "epoch: 1940, loss = 0.6969\n",
      "epoch: 1960, loss = 0.6923\n",
      "epoch: 1980, loss = 0.6877\n",
      "epoch: 2000, loss = 0.6831\n",
      "epoch: 2020, loss = 0.6786\n",
      "epoch: 2040, loss = 0.6740\n",
      "epoch: 2060, loss = 0.6695\n",
      "epoch: 2080, loss = 0.6649\n",
      "epoch: 2100, loss = 0.6603\n",
      "epoch: 2120, loss = 0.6558\n",
      "epoch: 2140, loss = 0.6512\n",
      "epoch: 2160, loss = 0.6467\n",
      "epoch: 2180, loss = 0.6421\n",
      "epoch: 2200, loss = 0.6376\n",
      "epoch: 2220, loss = 0.6330\n",
      "epoch: 2240, loss = 0.6285\n",
      "epoch: 2260, loss = 0.6239\n",
      "epoch: 2280, loss = 0.6194\n",
      "epoch: 2300, loss = 0.6148\n",
      "epoch: 2320, loss = 0.6103\n",
      "epoch: 2340, loss = 0.6058\n",
      "epoch: 2360, loss = 0.6012\n",
      "epoch: 2380, loss = 0.5967\n",
      "epoch: 2400, loss = 0.5922\n",
      "epoch: 2420, loss = 0.5877\n",
      "epoch: 2440, loss = 0.5832\n",
      "epoch: 2460, loss = 0.5787\n",
      "epoch: 2480, loss = 0.5742\n",
      "epoch: 2500, loss = 0.5698\n",
      "epoch: 2520, loss = 0.5653\n",
      "epoch: 2540, loss = 0.5609\n",
      "epoch: 2560, loss = 0.5565\n",
      "epoch: 2580, loss = 0.5521\n",
      "epoch: 2600, loss = 0.5477\n",
      "epoch: 2620, loss = 0.5433\n",
      "epoch: 2640, loss = 0.5389\n",
      "epoch: 2660, loss = 0.5346\n",
      "epoch: 2680, loss = 0.5303\n",
      "epoch: 2700, loss = 0.5260\n",
      "epoch: 2720, loss = 0.5217\n",
      "epoch: 2740, loss = 0.5174\n",
      "epoch: 2760, loss = 0.5132\n",
      "epoch: 2780, loss = 0.5089\n",
      "epoch: 2800, loss = 0.5047\n",
      "epoch: 2820, loss = 0.5006\n",
      "epoch: 2840, loss = 0.4964\n",
      "epoch: 2860, loss = 0.4923\n",
      "epoch: 2880, loss = 0.4882\n",
      "epoch: 2900, loss = 0.4841\n",
      "epoch: 2920, loss = 0.4801\n",
      "epoch: 2940, loss = 0.4760\n",
      "epoch: 2960, loss = 0.4720\n",
      "epoch: 2980, loss = 0.4681\n",
      "epoch: 3000, loss = 0.4641\n",
      "epoch: 3020, loss = 0.4602\n",
      "epoch: 3040, loss = 0.4563\n",
      "epoch: 3060, loss = 0.4524\n",
      "epoch: 3080, loss = 0.4486\n",
      "epoch: 3100, loss = 0.4448\n",
      "epoch: 3120, loss = 0.4410\n",
      "epoch: 3140, loss = 0.4373\n",
      "epoch: 3160, loss = 0.4336\n",
      "epoch: 3180, loss = 0.4299\n",
      "epoch: 3200, loss = 0.4263\n",
      "epoch: 3220, loss = 0.4227\n",
      "epoch: 3240, loss = 0.4191\n",
      "epoch: 3260, loss = 0.4155\n",
      "epoch: 3280, loss = 0.4120\n",
      "epoch: 3300, loss = 0.4085\n",
      "epoch: 3320, loss = 0.4050\n",
      "epoch: 3340, loss = 0.4016\n",
      "epoch: 3360, loss = 0.3982\n",
      "epoch: 3380, loss = 0.3948\n",
      "epoch: 3400, loss = 0.3914\n",
      "epoch: 3420, loss = 0.3881\n",
      "epoch: 3440, loss = 0.3848\n",
      "epoch: 3460, loss = 0.3815\n",
      "epoch: 3480, loss = 0.3783\n",
      "epoch: 3500, loss = 0.3751\n",
      "epoch: 3520, loss = 0.3719\n",
      "epoch: 3540, loss = 0.3687\n",
      "epoch: 3560, loss = 0.3656\n",
      "epoch: 3580, loss = 0.3625\n",
      "epoch: 3600, loss = 0.3594\n",
      "epoch: 3620, loss = 0.3564\n",
      "epoch: 3640, loss = 0.3534\n",
      "epoch: 3660, loss = 0.3504\n",
      "epoch: 3680, loss = 0.3475\n",
      "epoch: 3700, loss = 0.3446\n",
      "epoch: 3720, loss = 0.3417\n",
      "epoch: 3740, loss = 0.3388\n",
      "epoch: 3760, loss = 0.3360\n",
      "epoch: 3780, loss = 0.3332\n",
      "epoch: 3800, loss = 0.3304\n",
      "epoch: 3820, loss = 0.3277\n",
      "epoch: 3840, loss = 0.3250\n",
      "epoch: 3860, loss = 0.3223\n",
      "epoch: 3880, loss = 0.3196\n",
      "epoch: 3900, loss = 0.3170\n",
      "epoch: 3920, loss = 0.3144\n",
      "epoch: 3940, loss = 0.3118\n",
      "epoch: 3960, loss = 0.3093\n",
      "epoch: 3980, loss = 0.3068\n",
      "epoch: 4000, loss = 0.3043\n",
      "epoch: 4020, loss = 0.3018\n",
      "epoch: 4040, loss = 0.2994\n",
      "epoch: 4060, loss = 0.2970\n",
      "epoch: 4080, loss = 0.2946\n",
      "epoch: 4100, loss = 0.2922\n",
      "epoch: 4120, loss = 0.2899\n",
      "epoch: 4140, loss = 0.2876\n",
      "epoch: 4160, loss = 0.2853\n",
      "epoch: 4180, loss = 0.2830\n",
      "epoch: 4200, loss = 0.2808\n",
      "epoch: 4220, loss = 0.2786\n",
      "epoch: 4240, loss = 0.2764\n",
      "epoch: 4260, loss = 0.2743\n",
      "epoch: 4280, loss = 0.2721\n",
      "epoch: 4300, loss = 0.2700\n",
      "epoch: 4320, loss = 0.2679\n",
      "epoch: 4340, loss = 0.2659\n",
      "epoch: 4360, loss = 0.2638\n",
      "epoch: 4380, loss = 0.2618\n",
      "epoch: 4400, loss = 0.2598\n",
      "epoch: 4420, loss = 0.2578\n",
      "epoch: 4440, loss = 0.2559\n",
      "epoch: 4460, loss = 0.2540\n",
      "epoch: 4480, loss = 0.2521\n",
      "epoch: 4500, loss = 0.2502\n",
      "epoch: 4520, loss = 0.2483\n",
      "epoch: 4540, loss = 0.2465\n",
      "epoch: 4560, loss = 0.2446\n",
      "epoch: 4580, loss = 0.2428\n",
      "epoch: 4600, loss = 0.2411\n",
      "epoch: 4620, loss = 0.2393\n",
      "epoch: 4640, loss = 0.2376\n",
      "epoch: 4660, loss = 0.2358\n",
      "epoch: 4680, loss = 0.2341\n",
      "epoch: 4700, loss = 0.2324\n",
      "epoch: 4720, loss = 0.2308\n",
      "epoch: 4740, loss = 0.2291\n",
      "epoch: 4760, loss = 0.2275\n",
      "epoch: 4780, loss = 0.2259\n",
      "epoch: 4800, loss = 0.2243\n",
      "epoch: 4820, loss = 0.2227\n",
      "epoch: 4840, loss = 0.2211\n",
      "epoch: 4860, loss = 0.2196\n",
      "epoch: 4880, loss = 0.2181\n",
      "epoch: 4900, loss = 0.2165\n",
      "epoch: 4920, loss = 0.2151\n",
      "epoch: 4940, loss = 0.2136\n",
      "epoch: 4960, loss = 0.2121\n",
      "epoch: 4980, loss = 0.2107\n",
      "epoch: 5000, loss = 0.2092\n",
      "epoch: 5020, loss = 0.2078\n",
      "epoch: 5040, loss = 0.2064\n",
      "epoch: 5060, loss = 0.2051\n",
      "epoch: 5080, loss = 0.2037\n",
      "epoch: 5100, loss = 0.2023\n",
      "epoch: 5120, loss = 0.2010\n",
      "epoch: 5140, loss = 0.1997\n",
      "epoch: 5160, loss = 0.1984\n",
      "epoch: 5180, loss = 0.1971\n",
      "epoch: 5200, loss = 0.1958\n",
      "epoch: 5220, loss = 0.1945\n",
      "epoch: 5240, loss = 0.1933\n",
      "epoch: 5260, loss = 0.1920\n",
      "epoch: 5280, loss = 0.1908\n",
      "epoch: 5300, loss = 0.1896\n",
      "epoch: 5320, loss = 0.1884\n",
      "epoch: 5340, loss = 0.1872\n",
      "epoch: 5360, loss = 0.1860\n",
      "epoch: 5380, loss = 0.1849\n",
      "epoch: 5400, loss = 0.1837\n",
      "epoch: 5420, loss = 0.1826\n",
      "epoch: 5440, loss = 0.1815\n",
      "epoch: 5460, loss = 0.1804\n",
      "epoch: 5480, loss = 0.1793\n",
      "epoch: 5500, loss = 0.1782\n",
      "epoch: 5520, loss = 0.1771\n",
      "epoch: 5540, loss = 0.1761\n",
      "epoch: 5560, loss = 0.1750\n",
      "epoch: 5580, loss = 0.1740\n",
      "epoch: 5600, loss = 0.1729\n",
      "epoch: 5620, loss = 0.1719\n",
      "epoch: 5640, loss = 0.1709\n",
      "epoch: 5660, loss = 0.1699\n",
      "epoch: 5680, loss = 0.1689\n",
      "epoch: 5700, loss = 0.1680\n",
      "epoch: 5720, loss = 0.1670\n",
      "epoch: 5740, loss = 0.1660\n",
      "epoch: 5760, loss = 0.1651\n",
      "epoch: 5780, loss = 0.1642\n",
      "epoch: 5800, loss = 0.1632\n",
      "epoch: 5820, loss = 0.1623\n",
      "epoch: 5840, loss = 0.1614\n",
      "epoch: 5860, loss = 0.1605\n",
      "epoch: 5880, loss = 0.1596\n",
      "epoch: 5900, loss = 0.1587\n",
      "epoch: 5920, loss = 0.1579\n",
      "epoch: 5940, loss = 0.1570\n",
      "epoch: 5960, loss = 0.1562\n",
      "epoch: 5980, loss = 0.1553\n",
      "epoch: 6000, loss = 0.1545\n",
      "epoch: 6020, loss = 0.1537\n",
      "epoch: 6040, loss = 0.1528\n",
      "epoch: 6060, loss = 0.1520\n",
      "epoch: 6080, loss = 0.1512\n",
      "epoch: 6100, loss = 0.1504\n",
      "epoch: 6120, loss = 0.1496\n",
      "epoch: 6140, loss = 0.1489\n",
      "epoch: 6160, loss = 0.1481\n",
      "epoch: 6180, loss = 0.1473\n",
      "epoch: 6200, loss = 0.1466\n",
      "epoch: 6220, loss = 0.1458\n",
      "epoch: 6240, loss = 0.1451\n",
      "epoch: 6260, loss = 0.1443\n",
      "epoch: 6280, loss = 0.1436\n",
      "epoch: 6300, loss = 0.1429\n",
      "epoch: 6320, loss = 0.1422\n",
      "epoch: 6340, loss = 0.1415\n",
      "epoch: 6360, loss = 0.1408\n",
      "epoch: 6380, loss = 0.1401\n",
      "epoch: 6400, loss = 0.1394\n",
      "epoch: 6420, loss = 0.1387\n",
      "epoch: 6440, loss = 0.1380\n",
      "epoch: 6460, loss = 0.1374\n",
      "epoch: 6480, loss = 0.1367\n",
      "epoch: 6500, loss = 0.1360\n",
      "epoch: 6520, loss = 0.1354\n",
      "epoch: 6540, loss = 0.1348\n",
      "epoch: 6560, loss = 0.1341\n",
      "epoch: 6580, loss = 0.1335\n",
      "epoch: 6600, loss = 0.1329\n",
      "epoch: 6620, loss = 0.1322\n",
      "epoch: 6640, loss = 0.1316\n",
      "epoch: 6660, loss = 0.1310\n",
      "epoch: 6680, loss = 0.1304\n",
      "epoch: 6700, loss = 0.1298\n",
      "epoch: 6720, loss = 0.1292\n",
      "epoch: 6740, loss = 0.1286\n",
      "epoch: 6760, loss = 0.1280\n",
      "epoch: 6780, loss = 0.1275\n",
      "epoch: 6800, loss = 0.1269\n",
      "epoch: 6820, loss = 0.1263\n",
      "epoch: 6840, loss = 0.1258\n",
      "epoch: 6860, loss = 0.1252\n",
      "epoch: 6880, loss = 0.1247\n",
      "epoch: 6900, loss = 0.1241\n",
      "epoch: 6920, loss = 0.1236\n",
      "epoch: 6940, loss = 0.1231\n",
      "epoch: 6960, loss = 0.1225\n",
      "epoch: 6980, loss = 0.1220\n",
      "epoch: 7000, loss = 0.1215\n",
      "epoch: 7020, loss = 0.1210\n",
      "epoch: 7040, loss = 0.1204\n",
      "epoch: 7060, loss = 0.1199\n",
      "epoch: 7080, loss = 0.1194\n",
      "epoch: 7100, loss = 0.1189\n",
      "epoch: 7120, loss = 0.1184\n",
      "epoch: 7140, loss = 0.1179\n",
      "epoch: 7160, loss = 0.1174\n",
      "epoch: 7180, loss = 0.1170\n",
      "epoch: 7200, loss = 0.1165\n",
      "epoch: 7220, loss = 0.1160\n",
      "epoch: 7240, loss = 0.1155\n",
      "epoch: 7260, loss = 0.1151\n",
      "epoch: 7280, loss = 0.1146\n",
      "epoch: 7300, loss = 0.1141\n",
      "epoch: 7320, loss = 0.1137\n",
      "epoch: 7340, loss = 0.1132\n",
      "epoch: 7360, loss = 0.1128\n",
      "epoch: 7380, loss = 0.1123\n",
      "epoch: 7400, loss = 0.1119\n",
      "epoch: 7420, loss = 0.1115\n",
      "epoch: 7440, loss = 0.1110\n",
      "epoch: 7460, loss = 0.1106\n",
      "epoch: 7480, loss = 0.1102\n",
      "epoch: 7500, loss = 0.1097\n",
      "epoch: 7520, loss = 0.1093\n",
      "epoch: 7540, loss = 0.1089\n",
      "epoch: 7560, loss = 0.1085\n",
      "epoch: 7580, loss = 0.1081\n",
      "epoch: 7600, loss = 0.1077\n",
      "epoch: 7620, loss = 0.1072\n",
      "epoch: 7640, loss = 0.1068\n",
      "epoch: 7660, loss = 0.1064\n",
      "epoch: 7680, loss = 0.1061\n",
      "epoch: 7700, loss = 0.1057\n",
      "epoch: 7720, loss = 0.1053\n",
      "epoch: 7740, loss = 0.1049\n",
      "epoch: 7760, loss = 0.1045\n",
      "epoch: 7780, loss = 0.1041\n",
      "epoch: 7800, loss = 0.1037\n",
      "epoch: 7820, loss = 0.1034\n",
      "epoch: 7840, loss = 0.1030\n",
      "epoch: 7860, loss = 0.1026\n",
      "epoch: 7880, loss = 0.1023\n",
      "epoch: 7900, loss = 0.1019\n",
      "epoch: 7920, loss = 0.1015\n",
      "epoch: 7940, loss = 0.1012\n",
      "epoch: 7960, loss = 0.1008\n",
      "epoch: 7980, loss = 0.1005\n",
      "epoch: 8000, loss = 0.1001\n",
      "epoch: 8020, loss = 0.0998\n",
      "epoch: 8040, loss = 0.0994\n",
      "epoch: 8060, loss = 0.0991\n",
      "epoch: 8080, loss = 0.0987\n",
      "epoch: 8100, loss = 0.0984\n",
      "epoch: 8120, loss = 0.0980\n",
      "epoch: 8140, loss = 0.0977\n",
      "epoch: 8160, loss = 0.0974\n",
      "epoch: 8180, loss = 0.0970\n",
      "epoch: 8200, loss = 0.0967\n",
      "epoch: 8220, loss = 0.0964\n",
      "epoch: 8240, loss = 0.0961\n",
      "epoch: 8260, loss = 0.0957\n",
      "epoch: 8280, loss = 0.0954\n",
      "epoch: 8300, loss = 0.0951\n",
      "epoch: 8320, loss = 0.0948\n",
      "epoch: 8340, loss = 0.0945\n",
      "epoch: 8360, loss = 0.0942\n",
      "epoch: 8380, loss = 0.0939\n",
      "epoch: 8400, loss = 0.0936\n",
      "epoch: 8420, loss = 0.0933\n",
      "epoch: 8440, loss = 0.0930\n",
      "epoch: 8460, loss = 0.0927\n",
      "epoch: 8480, loss = 0.0924\n",
      "epoch: 8500, loss = 0.0921\n",
      "epoch: 8520, loss = 0.0918\n",
      "epoch: 8540, loss = 0.0915\n",
      "epoch: 8560, loss = 0.0912\n",
      "epoch: 8580, loss = 0.0909\n",
      "epoch: 8600, loss = 0.0906\n",
      "epoch: 8620, loss = 0.0903\n",
      "epoch: 8640, loss = 0.0901\n",
      "epoch: 8660, loss = 0.0898\n",
      "epoch: 8680, loss = 0.0895\n",
      "epoch: 8700, loss = 0.0892\n",
      "epoch: 8720, loss = 0.0889\n",
      "epoch: 8740, loss = 0.0887\n",
      "epoch: 8760, loss = 0.0884\n",
      "epoch: 8780, loss = 0.0881\n",
      "epoch: 8800, loss = 0.0879\n",
      "epoch: 8820, loss = 0.0876\n",
      "epoch: 8840, loss = 0.0873\n",
      "epoch: 8860, loss = 0.0871\n",
      "epoch: 8880, loss = 0.0868\n",
      "epoch: 8900, loss = 0.0865\n",
      "epoch: 8920, loss = 0.0863\n",
      "epoch: 8940, loss = 0.0860\n",
      "epoch: 8960, loss = 0.0858\n",
      "epoch: 8980, loss = 0.0855\n",
      "epoch: 9000, loss = 0.0853\n",
      "epoch: 9020, loss = 0.0850\n",
      "epoch: 9040, loss = 0.0848\n",
      "epoch: 9060, loss = 0.0845\n",
      "epoch: 9080, loss = 0.0843\n",
      "epoch: 9100, loss = 0.0840\n",
      "epoch: 9120, loss = 0.0838\n",
      "epoch: 9140, loss = 0.0835\n",
      "epoch: 9160, loss = 0.0833\n",
      "epoch: 9180, loss = 0.0831\n",
      "epoch: 9200, loss = 0.0828\n",
      "epoch: 9220, loss = 0.0826\n",
      "epoch: 9240, loss = 0.0824\n",
      "epoch: 9260, loss = 0.0821\n",
      "epoch: 9280, loss = 0.0819\n",
      "epoch: 9300, loss = 0.0817\n",
      "epoch: 9320, loss = 0.0814\n",
      "epoch: 9340, loss = 0.0812\n",
      "epoch: 9360, loss = 0.0810\n",
      "epoch: 9380, loss = 0.0808\n",
      "epoch: 9400, loss = 0.0805\n",
      "epoch: 9420, loss = 0.0803\n",
      "epoch: 9440, loss = 0.0801\n",
      "epoch: 9460, loss = 0.0799\n",
      "epoch: 9480, loss = 0.0797\n",
      "epoch: 9500, loss = 0.0794\n",
      "epoch: 9520, loss = 0.0792\n",
      "epoch: 9540, loss = 0.0790\n",
      "epoch: 9560, loss = 0.0788\n",
      "epoch: 9580, loss = 0.0786\n",
      "epoch: 9600, loss = 0.0784\n",
      "epoch: 9620, loss = 0.0782\n",
      "epoch: 9640, loss = 0.0780\n",
      "epoch: 9660, loss = 0.0777\n",
      "epoch: 9680, loss = 0.0775\n",
      "epoch: 9700, loss = 0.0773\n",
      "epoch: 9720, loss = 0.0771\n",
      "epoch: 9740, loss = 0.0769\n",
      "epoch: 9760, loss = 0.0767\n",
      "epoch: 9780, loss = 0.0765\n",
      "epoch: 9800, loss = 0.0763\n",
      "epoch: 9820, loss = 0.0761\n",
      "epoch: 9840, loss = 0.0759\n",
      "epoch: 9860, loss = 0.0757\n",
      "epoch: 9880, loss = 0.0755\n",
      "epoch: 9900, loss = 0.0754\n",
      "epoch: 9920, loss = 0.0752\n",
      "epoch: 9940, loss = 0.0750\n",
      "epoch: 9960, loss = 0.0748\n",
      "epoch: 9980, loss = 0.0746\n",
      "epoch: 10000, loss = 0.0744\n"
     ]
    }
   ],
   "source": [
    "#Train the Model\n",
    "# ============================ step 5/6 training ============================    \n",
    "for epoch in range(num_epochs):\n",
    "    models.train()\n",
    "    optimizers.zero_grad()\n",
    "    # Forward pass\n",
    "    y_pred  = models(X_train)\n",
    "    # Compute Loss\n",
    "    loss = criterions(y_pred, y_train)\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizers.step()\n",
    "    if (epoch+1) % 20 == 0:                                         \n",
    "        # printing loss values on every 10 epochs to keep track\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, \n",
    "+ when you call `models(X_train)`, you automatically call `models.forward()` to propagate forward.\n",
    "+  Next, the loss is calculated. When `loss.backward()` is called, it computes the loss gradient with respect to the weights (of the layer). \n",
    "+ The weights are then updated by calling `optimizer.step()`. \n",
    "+ After this, the weights have to be emptied for the next iteration. So the `zero_grad()` method is called.\n",
    "\n",
    "The above code prints the loss at each 20th epoch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ step 7 Model Performance\n",
    "  \n",
    "Let us finally see the model accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9500\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = models(X_test)\n",
    "    y_pred = nn.Softmax(dim=1)(logits)\n",
    "    y_predicted_cls = y_pred.argmax(1)\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy: {acc.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94         9\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       0.88      1.00      0.93         7\n",
      "\n",
      "    accuracy                           0.95        20\n",
      "   macro avg       0.96      0.96      0.96        20\n",
      "weighted avg       0.96      0.95      0.95        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_predicted_cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB Assignment\n",
    "### Exercise 1 logistic regression (50 points )\n",
    "This exercise uses dataset digit01.csv , which has 13 columns, and the last column is the dependent variable. \n",
    "\n",
    "This part requires you to implement a `logistic regression` using the pytorch framework (defining a logistic regression class that inherits `nn.module`). To test your model, we provide a dataset `digit01.csv` which is in the **datasets folder**. This dataset requires you to divide the training set and the test set by yourself, and it is recommended that 80% of the training set and 20% of the test set be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2  Handwriting recognition with MLP(50 points )\n",
    "\n",
    "Like last week's lab , your task in this section is also about recognizing handwritten digits, but you are required to use MLP to complete the exercise. It is recommended that you define an MLP class, which is a subclass of `nn.module`.\n",
    "\n",
    "<font color='red' size=4>Note that your accuracy in this section will directly determine your score.</font>\n",
    "\n",
    "For this exercise we use the `minist` dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9cf8428aa180ee23632ed7df20f7a595edda7c60e668686876baf89d702ea1cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
